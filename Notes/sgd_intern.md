<font face='consolas'>

<div align=center>

## 盛冠达实习日志
</div>

> Background is cheap, show me the alpha.
> Talk is cheap, show me the code.
> Create alpha, and be an alpha.


---
<div align=center>

### A股领先滞后研究
</div>

### 1. 核心文献
#### **Paper: High Frequency Lead/lag Relationships Empirical facts**
#### **Paper: Ultra High Frequency Statistical Arbitrage Across International Index Futures**

基本是HFT里面讲leadlag的引用量最高的文章了，第二篇是完全基于第一篇开发的超高频的一个跨市场回测。分别是2012年、2013年的文章，引用量有70多。有点老旧了，但是方法非常经典，现在研究的也不是很多，大多数都是在一些国内的博士论文里面瞎水文章水出来的，往往都是在研究期现货的leadlag，然而交易操作难度很大，纸上谈兵没个卵用。文章有点早，也所幸有人开发了HFHD这个包，快速的计算HY互相关，给我的工作带来很大的帮助，不然我可能三个月就去自己手写然后优化一个小工具去了，根本做不到策略。当然他的包也不是完善完整的，直接运行也会因为数据格式的问题报错，我在里面debug了，加了自己的函数和代码，方便一扫多的需求使用。

**文章结论**
- 期货和对应的股票有高度不对称的互相关信号。
- 流动性更强的资产（相邻两笔成交的duration时差短，bid-ask spread小，波动率低，换手率高）更倾向于领先流动性低的资产；然而相同流动性的asset的互相关性更强。
- Lead-lag关系难以捕捉，日内的lead-lag并不持续，具有日内周期性，比如宏观数据的公布或者US开盘的时候会比较明显。
- 数据越高频lead-lag关系越明显，使用leader的历史数据去估计lagger的next midquote variation，做一个Midquote的涨跌预测（二分类问题）可以达到60%的accuracy，远高于lagger自身的历史数据。而当从tick data变为minute data，频率降低时，lead-lag relation消失。
- 然而单凭这样的预测去下市价单，会吃不过bid-ask spread赚不了钱。

金融市场最著名的APT，no arbitrage若成立（即可以搭建zero-cost portfolio实现在未来有非零的概率获得正收益），那此时股票和指数的价格序列画cross-correlation function（两个时序的不同lag trading days的）的结果应该是一个Dirac delta脉冲函数（顶点为1，其余为0）乘上两样资产的daily return的相关系数。除了lag=0处以外其余部分应该是类似white noise，绝对值范围在2/sqrt(n)以内。
日度的数据基本没有lead-lag relation，但高频的数据就有可能。

数据：
- Trade和quote（best bid and ask price and quantity），没有用到orderbook的多级行情。Quote往往多于trade，数据合并时以trade为基准，去匹配向前的最近的quote的数据。sampling schedule是依据高频的trade（resample time）去匹配发生的quote的数据，也就是说有的quote数据会用不到。如果两笔quote都夹在两个trade中间，那只会取离后一个trade最近的那笔trade。相当于observation time就是我们的quote的time，然后sample time就是我们的trade的time，然后quote根据trade去resample，得到的是一个子集，可以叫做pre-tick time或者叫做sampled quote time。后续在处理的时候没有直接使用原先quote的observation time，而是使用的tick time，也就是mid-quote有变化的时间，所以频次会比原先的quote time要低，然后使用这个quote time再去做resample，得到resampled quote time。这样得到的resampled quote time更一定满足相邻的mid-quote都是不一样的。
- 同时把相同时间的成交给aggregate，成交量相加，成交价换成VWAP，即把相同时间戳的成交量加和，成交价聚合成VWAP，保证乘积（成交额）不变
- 研究股票之间、股票与对应期货的近月合约。
- 每天舍弃开盘半小时和收盘前半小时的数据（不过注意这是法国交易所，早上九点到下午五点半）
- 只考虑常规交易，不考虑大宗交易（block trades）或OTC（场外交易）
- 对于不同交易所的资产，不考虑交易所的时间延迟

**Liquidity衡量参数**
- Trade里相邻两笔成交时差的平均值delta_t
- Average tick size/midquote，tick size是the smallest possible price fluctuation for any particular asset，也就是价格变动的最小单位，除以midquote的，再取平均值
- Bid-ask spread除以tick size的平均值，即(ask-bid)除以价格最小变动单位，再取平均值。这里的spread应该没有除以mid quote，只是ask-bid
- Ask-bid等于最小价格变动单位的次数，表示为%
- Trade through的次数（trade price低于当前best bid price的次数），表示为%
- 日度波动率的一个模拟值：相邻midquote的变化量绝对值除以tick size的平均值。这个比值越大，说明midquote变得越频繁，价格波动越大
- Turnover per trade，每笔交易换手率，price * trade_qty
所有需要算average的指标均以日度为单位独立计算，然后对所有天求平均，相当于每笔trade或者quote取平均，然后再对days取平均。

**Hayashi-Yoshida cross-correlation estimator**
1.	是一个unbiased and consistent的covariance estimator，主要用于处理asynchronous trading异步交易问题以及有microstructure noise的问题，也就是会有intraday variations in scale of noise，而且这种noise下的observation是离散的，不同步的时刻发生的。同时HY估计使用到了所有的tick data，可以理论上衡量最精确的lag time（异步交易难以衡量延后时间）
2.	paper《covariance structure》里面提出的这个HY关联函数，证明了在恒定波动率和correlation下这个estimator是consistent的，也就是数据量足够大的时候会逼近真实值
3.	HY关联函数的lag版本，就是这个paper用的互相关函数。maximize这个函数可以得到opt_l，这个opt_l最大化了相关性，记为两个时序的lead-lag time，本文第一看函数的不对称性进而判断谁是leader谁是lagger，第二看LLR函数的最大值以及所在点判断lead-lag关系的强度和最显著的lead-lag time。
4.	互相关的结果如果是高度不对称的，也就是说如果LLR远大于1，说明存在lead-lag关系，且X leads Y比Y leads X更加accurate。
5.	注意这个lag order L，这个L是只对observation time使用的平移。平移后重新计算overlap。

**模型**
- Ito process里的X，Y对应midquote，使用tick time而并非trade time，目的是做一个二分类问题。目的是估计均价在下一个tick time的变化情况，比如如果估计的下一个tick time这个涨的bps超过了比如一个平均的spread，那就买入，然后等到下一个tick time发生的时候就卖出套利。当然，这里就有两个问题，且不说不能在midquote交易本就是一个理想的情况。第一，下一个tick time是未来信息，我们需要估计。论文里使用的是当前时间加上相邻tick time的duration的平均值作为估计的。第二，二分类但我们做估计的时候似乎并不能使用其他机器学习模型，而只能使用线性回归，因为我们HY估计量的推导基础就建立在线性预测的基础上，如果用其他的预测方式可能不是很合理，而且机器学习只有leader的收益率也不是很科学，如果说延申的话可以考虑把leader的收益率和lagger自身的其他订单簿信息结合起来送到模型里面去预测。

**Intraday profile**
- 研究lead-lag intraday profile，发现lead-lag程度和news以及开盘高度相关。Profile本身和资产也相关。股票和股票，股票和期货算的intraday profile完全不同。Profile就是比如每五分钟取样，然后截面取平均，再在时序上相同时间段取平均。
- 计算5-min profile，日内做截面平均（所有的pair的LLR，然后对时序平均。
- 用来计算Intraday profile的lag grid只使用少于一分钟的，因为5min的profile比如9:30到9:35，我们就只能用这5min中的切片数据去计算correlation，如果lag太多，那么nan数据过多计算的corr就不准确。处理股票数据就发现，因为我们如果只取相邻midquote有变化的时间点，那5min的数据就更少了，计算出来的correlation没有意义。

**为什么不用pearson？**
1.	异常数据会干扰评估的结果，而高频数据里面存在microstructure noise
2.	假设数据都是同方差的，而这显然对intraday数据不适合
3.	相关性是全局同步性的快照测量法，所以不能提供两个时序信号的方向上的信息，也就是不能知道谁是leader谁是lagger。当然可以使用sliding window去计算局部的同步性，算一个滑动的平均，但无论如何都无法衡量有时间滞后的相关性
4.	只能衡量linear correlation
5.	使用TLCC（时间滞后互相关），可以计算两个时序的方向性，比如lead-lag关系，注意lead-lag关系并不能推出causal relationship，只能体现哪个时序更早的反应了信息。

**两个时序之间的检验**
1.	单位根检验，不平稳做差分
2.	协整检验cointegration，看两个时序是否有长期的关系，没有就用VECM看看短期的关系
3.	Granger因果检验，看X的过去值能否可以更好的预测Y未来的值
4.	做完granger可以做IR脉冲反应函数，这个图像一般最后是收敛的

**Granger causality test是什么**
在时间序列情形下，两个经济变量X、Y之间的格兰杰因果关系定义为：若在包含了变量X、Y的过去信息的条件下，对变量Y的预测效果要优于只单独由Y的过去信息对Y进行的预测效果，即变量X有助于解释变量Y的将来变化，则认为变量X是引致变量Y的格兰杰原因。虽然说是检验causality的但实际上只是在检验lead-lag关系，注意lead-lag关系并不能推出causal relationship，只能体现哪个时序更早的反应了信息。granger的基础是回归分析中的自回归模型，回归分析得到的是不同变量的同期的相关性，自回归模型得到的是同一变量不同期的相关性。
这个概念本身和lead-lag很接近，但一般只用在经济学变量的领域，同时需要时序是stationary的，没有考虑干扰因素的影响，也没有考虑时序之间非线性的相互关系。

**Epps effect是什么**
correlation estimates tend to decrease when sampling is done at high frequencies。
简言之就是宏观上高度相关的两个品种在微观上，比如tick级别是完全不一样的。很好理解，双胞胎长得一模一样，放到微观里面看细胞也是完全不一样的。也就是说，A,B两个高度相关的品种的价格时间序列，使用不同频率切片的数据去求相关系数，其结果是完全不一样的。切片频率越高，相关系数越低；切片频率越低，相关系数越高。babyquant测试期货的对数收益率，日内都是不平稳的，但平滑之后，或者每隔间距取一个点，再把时间拉长增多数据，放到更加宏观的层面，就变得平稳了。

**问题**
- 时序的这个互相关函数和CNN的互相关函数有什么关系？
- Paper里面最后发现有lead-lag的显著的只有股票和对应的期货，之前A股发现的lead-lag会不会只是因为“错误的计算”而发现的呢？因为对于之前的previous-tick estimator，得到的总是更加liquid的资产在lead不是那么liquid的资产。
- Previous ticks计算公式需要resample，而这个HY estimator公式本身看是不需要的，那到底需不需要？如果真的不需要为什么还要把quote给按照trade去sample了？
- 如果要找两个股票的lead-lag，那这两个股票的选取标准是什么？我们总不可能对所有的股票算pair吧？高频的本来就计算量爆炸。像pair trading好歹还可以验证比如correlation或者什么协整关系去选pair。哦懂了，根据LLR去画lead-lag network，画一个影响力网络，然后去看权重大的边。选取的对象可以是比如同一个板块同一个概念的股票吗？
- 具体怎么通过lead-lag关系去生成交易策略？这个lead-lag关系也会随时更迭，那我们是比如固定一个周期去重新画影响力网络然后选最新的pair？比如每天都重新评估一次然后选影响力最高的若干pair，下一个交易日去交易一天？至少文章里面的所有预测，然后下市价单都是亏钱的，当然可能是他选的那个pair不够好，但是总感觉有很大提升的空间，比如你不是每次预测涨就买，是不是应该超过一定阈值再买等等，以及他自己说的resample的时候用更宽的时间频率，比如不用一个tick（即midquote变化的时间间隔），而是用多个tick之后进行采样，效果可能会好？这样的话也有tradeoff，持有时间会变长一些，手续费吃得少，但是预测的准确率也会下降，因为时间拉长了。
- 这个能不能来搞股票和股指期货？
- 这个HY ESTIMATOR的计算能不能实现向量求解？本来数据量就很大，for循环太恐怖了吧。本身按照概念，是每一个相邻的都要去匹配和他有交集的，然后去相加。

**股票T0交易**
A股实行的是T+1交易制度，但这并不妨碍做T0。期货美股这些是做高频日内交易，指的是日初开仓，然后中间各种操作，不同的频率对应日内短线（持有几分钟），日内大波段（只抓当日的主趋势）还有日内中长线（持有一小时），然后日内交易需要当日清仓，相当于当日买当日卖。A股T+1不能当日买了之后当日卖出，而是针对那些我们打算长期持有的股票，而且是已经有底仓的股票。如果已经有底仓了，那我们就可以实现当日买当日卖，或者当日卖当日买。预期今天下跌，先卖后买维持仓位不变叫做“反T”，预期上涨后先买后卖维持仓位不变锁定利润就是“正T”。差价小，需要低手续费，印花税千分之一，一般要万二，千1.4的双边费用已经够高了。而lead-lag本就是T0最著名的策略，有点像事件策略，平常我们看机构买了我们跟，高管增持了我们跟，这就是事件策略，而T0短线里面，比如同一个板块，同一个概念和逻辑，利用板块联动的时间差做T，是最为核心的方法，其实就是用的这种lead-lag关系。
一天做T不宜次数太多，有确定性机会再做。尽量保持每日收盘的时候仓位不变，避免做隔日的T，更应该避免做隔周T，就是周五操作之后周一再完成一套闭环，周末信息的不确定性太大，不适合T0。同时T0一定要坚决的止盈止损，比如2%，也就是立刻恢复原来的仓位。



### 2. A股HY效果检验

我们使用中信证券600030和海通证券600837去扫全市场的A股，使用一天的快照数据，使用的是2022-06-01的数据。我初步截取了早上开盘9:30后到下午14:57的数据计算相关性。预测对象是midquote的变化情况，同样是使用tick time，也就是相邻Midquote发生变化的时点。我们会剔除掉tick time时间过少的股票（比如数量少于50条） 

由于是一天对四千多只股票，我们对股票做并行，使用24个核（后续如果是多股票多天数那需要并行的对象可能发生变化，整个全市场扫描计算HY estimator和LLR的过程可以被看做是一个三循环，先对天数循环，每天计算四千扫四千的股票的相关性也就是对股票pair做循环，最后是对grid做循环以使用HY相关性计算LLR。从体量来看，论文里面使用的是几年的数据但是他只考虑了几个pair，而我们交易要用的必定是大量的pair，而窗口根据算力不会取太多天，比如20天，甚至更短比如一星期，直观也容易理解，往往lead-lag pair未必是长期存在的现象，对任意的一个pair，你把时间拉长到一年，那计算的HY相关性的平均值都会很小，LLR的平均值都会接近一，因为我们本来用的就是高频的数据，很少会有两样资产在比较长的一段时间内始终存在lead-lag。所以时间窗口太长往往也不会得到很有用的pair，甚至会错过很多短期lead-lag很明显的pair。但当然，时间太短像一天两天也肯定不行，就像我现在做的一天的情况，数据量太少，本身除去midquote不变化的tick time就已经比较少了（比如海通的3s快照在只考虑tick time的情况下只剩下了250条数据），数据少了本身correlation的准确性就会影响，因为HY估计值的unbiased还好说，但是consistency就发挥不出来因为数据太少，想想我们之前复现的Brownian motion的时候那个数据点的数量都是上万级别的，当然计算时间也特别长。这就导致了现在可以看出日内的LLR的情况，往往LLR最高和最低的都是些极其不对称的比如什么开盘涨停跌停一类的，而榜首出现完全不是证券类的股票也非常常见，比如出现什么医药类或者电力类的股票。因为一天内的数据太少，价格曲线本身就变化多端，这也很正常。而这是无法通过增加grid的细粒度缓解的，因为grid的细粒度仍然只是在尽可能的返回日内的一个lead-lag的不对称性的平均情况。你一天内价格序列就是很不对称那你grid取的再细也没有意义，不能指望说把grid取细从而实现让LLR更接近一，不可能的。目前的数据由于是3s级别的，（不一样，看到很多不同的tick的快照细度，有的是3s的有的5s还有按分钟的），所以影响就是lag取0.1和0.2一定是一样的，因为不会产生更多错位相交的interval，然而他们和0的lag都不一样，**因为领先0.1s或者落后0.1s显然会产生新的错位相交的interval**（比如两个快照数据同频的情况下）。因此我使用的grid一开始是0.5s为单位的，后来就逐渐的拉宽拉长，最大到300s，细粒度已经相当足够了，相当于每个LLR我都要计算71个HY估计值，lead的35个，lag的35个，还有0的一个。

所以，如果这个LLR想用到实盘，理想的情况是算力足够支持，可以实现外层是比如20天左右，然后内层四千乘四千，然后grid根据具体的数据频次而定，如果是更高频的比如一秒钟两个数据（在0.5和1的位置），那grid颗粒度要更细致一点以确保反应情况，比如0.1和0.2是一样的，但是0和0.1和0.5和0.6都会是不一样的，而如果一秒两个数据而且并不是在0.5和1的时间点，而是一个随机的时点，那这个时候0，0.1，0.2，0.3，0.4，0.5……这些的结果都会不一样，这个时候就需要用到文章里面的那个grid去做了，文章里面的数据应该也是这样的。那么这个时候考虑并行的问题，不出意外还是得针对股票pair并行因为数量太大，而另外两个循环就尽量去做成向量化，包括HY可能的进一步的优化，做成向量话，包括grid的for循环使用map的迭代器去替代，包括天数也大概率可以这么做，或者一种想法是先把20天数据拼起来然后一块算，等等。但是目前的问题还是HY计算太慢，哪怕是使用tick time已经大大减少了数据量，基本上96个核并行的话海通筛后一天剩下两百多行，也需要30min，而中信如果是24核并行要3个小时。不敢想象怎么可能去四千刷四千，而且还是二十天，因为我现在已经在用dask/parallel在并行，grid的计算也是通过map的迭代器形式去算的，就算再加速HY函数向量化感觉也不现实，毕竟计算grid要几十个HY的事实摆在那里……

**目前的问题**
1.	A股的数据用什么时间段的？这个数据是怎么生成的？为什么下午三点以后还有，以及14：57的数据是什么情况，还有早上开盘前的数据是什么情况？
2.	Grid怎么取？如果是日内的，或者不同时间段的grid，应该取不一样的面板。要不要带端点？？不同时间段的grid频率应该不一样，比如半小时单位的和一天的，显然一天你的应该更多的grid因为不稳定。同时grid还要根据数据的频率来定。比如你3s的快照那就是grid取0和0.1会不一样，因为数据会错开，相同时间段的数据会错开，但是0.1和0.2和0.5是一样的。所以我暂时把grid按0.5来分组取了。
3.	涨跌停怎么处理？为什么开盘涨停的情况下这个midquote还能发生变化（如果不发生变化那么tick time的时间过少我会把它剔除掉），以及还有没有其他什么需要考虑的特殊情况，导致数据不正常需要提前筛选掉的pair？这么取数据的时间段是否合理？
4.	关于HY相关系数的计算：如果参与就算correlation的数据条目太少是应该剔除的，那少于多少剔除？（因为从HY的定义，他是unbiased和consistent，但是需要相邻的时间interval极其的逼近，意思就是说不仅需要我们的数据本身就极其高频以至于可以取模拟连续的情况，更要求在midquote变化筛选的tick time同样要满足数据比较高频以至于可以模拟连续的情况！而这个本身应该很困难，因为就算是极其高频的行情，如果不是流动性特别强的股票，你快照再频繁也没有用，因为相邻之间不会出现midquote变化，一样tick time会很稀疏。
5.	关于数据的问题，为什么000857在6月1号的成交量是0？哦因为是指数，指数不能直接买。那问题就是这个获取的stock_list里面，第一，为什么会有重复代码？第二，究竟怎么从里面取全A股的数据？删除掉ETF和指数的数据。我目前就是只筛选的股票代码开头为00、30、60、68、43、83、87的股票，包括什么沪深主板中小板新三板啥的，还有北交所的，但目前看来里面有指数，还有涨跌停，这些都需要处理！（比如去除指数和ETF，很可能数据库里面的指数和对应ETF的编号是一样的，那难道是去除所有duplicates就行了？）一样的可能是一只股票和一个指数，比如000933就是神火股份和中证医药，一个是股票一个是指数……所以可能我直接用server获取得到的就是指数……
6.	TMD相关性高的都是指数，跟指数lead-lag我也不能交易指数啊？？
7.	**选取tick time计算LLR会不会影响我们的投资策略**？因为文中这么做是因为他下面构建策略的时候是预测下一个tick time从而进行操作的。不过我想似乎也应该这么做，他下面给的结论是单纯的市价单没办法cover掉价差，可能的建议是去提高阈值，就是quote变化好几次才取一次，这样降低交易频率，但是accuracy也会下降，吃的价差也会少一些。那既然如此，我们是否应该用tick time？或者再采样松一点，比如隔三个tick再取time？不知道是否应该这么做，因为不清楚我们要构建的交易策略是怎么样的。当然每隔几个tick取是好的，因为计算量会成倍的减少！尤其是要扫过四千多只股票，那计算量会少很多。而且无论数据是多高频的，我都只取相邻midquote有变化的时点，那这计算量其实不会随数据变得高频而升高很多。但是问题就是，如果用tick time计算，比如LLR比较显著，然后我们使用了max lead的corr和对应发生的位置，虽然lead的是绝对的时间，但这个时间能用来实际的时间吗？好像也不影响，反正预测的是midquote发生变化的时候。
8.	**还有就是能否考虑intraday profile进行交易**？之前说比如我用过去20天的数据去算LLR，然后取比如平均最高的或者始终统计显著大于1的对子交易，但是我想A股有这个概念的东西，那lead lag的intraday profile应该会很明显，而且lead lag的关系在日内也很有可能发生变化，比如说一个股票在早盘开盘的时候引领了另一只，但是在下午的时候另一只反过来引领，这不是都有可能吗？所以是否可以考虑比如我用过去20天的9:30-10:00的情况去决定今天9:30-10:00交易的pair，然后每过半小时换一批？感觉这样会更科学，因为像海通的报告里面高频因子的有效性都是按什么全天，早收盘前，中午开盘后，盘中啥的分开测试，发现比如开盘后平均委买变化率选股是有效的等等。感觉是应该这么做的，这样的话数据可能会很少，要注意筛选，如果数据太少计算的correlation是不准确的，要筛掉，同时grid也要调整。但是这样也需要注意，就是数据量的问题，correlation这种东西肯定是数据量大了才会准确，然而这个intraday profile取半小时还行，但要是取比如5min，那数据量太少，尤其是使用tick time的话，就算我想研究开盘前也就是9:30前的一个lead lag情况，但是数据量太少了，虽然有半个小时，但是实际发现mid quote的变化的时间点就几个数据点，根本不足以计算correlation，更何况我们还要计算不同的lag grid下的correlation，那就更不准了。总体感觉策略优化到后期这个intraday还是很重要的，文献里面法国市场本就有intraday profile，而且他们还是中午没有收盘。A股市场中午还要收盘，Intraday profile应该会更明显。
9.	怎么选定pair的问题：再看看论文。除了一方面LLR非常大或者非常小的对子里面一大批都是指数以外，另一批经常会出现毫不相关的股票，比如我跑出来的是证券和医药。这也可以理解，这种leadlag在短期内可能显著，但是如果时间拉长就没有了，可能就只是那一天的统计上的巧合罢了。所以单单一天的LLR也许不可信。而如果比如我们计算了多天的LLR，那么反应的就是一个平均的情况，平均下来仍然有leadlag，那拿来预测是比较科学的情况，不会出现因为是短期的leadlag所以昨天有leadlag但今天没有的情况。当然，这只是其中一个问题。更重要的问题是，LLR就算取了平均，也只能反应一个平均的情况，那就是X leads Y比Y leads X更加显著。然而但依据LLR选pair显然没有意义，好比你一个涨停的股票和一个波动贼大的股票，那LLR是可能很不对称，但是他们correlation很低啊，那也同样不具备任何价值，意思就好比你LLR的分子分母都是极其小的数字，除法除出来一个比较decent的LLR，你觉得有意义吗？能拿来做prediction吗？没有意义啊！所以一定是需要看max lead/lag correlation以及对应发生的时间点的，这个东西，也是侯总用regression做不出来的东西，就是这个最佳的lead lag time。所以肯定是得结合看，**LLR只是一个相对强弱指标，不是绝对的，而corr才是绝对的指标。论文里说：Good candidates are close nodes (high correlation) with red links (high LLR). 也就是lead lag network里面越近的点代表correlation越高，correlation应该指的是本身不带leadlag的correlation，因为epps effect说的就是宏观走势一致的在微观不一致，就好比配对交易，我们做leadlag策略交易的对象一定是价格走势高度一致的但是高频的短期内并不一致的对象（我们期待在短时间内当时间从高频开始降低频率，序列继续维持一致）。当然感觉max_lead_corr或者max_lag_corr也应该考虑在内，而且这两个值发生的时点对交易策略的设计应该是有用的**。具体怎么用还没有想好。
10.	所以对应的问题是Leadlag network怎么画？最小生成树？以及需要同时考虑两个指标，correlation和LLR，甚至是同时考虑多个指标比如max_lead_corr和max_lag_corr要怎么抉择？ 
11.	还有就是程序的问题，就是除了可能的LLR的优化，还有就是我们并行需要输出什么结果，以及怎么样搞会更快一点，因为我怀疑存储到字典会减速，字段太冗长复杂了。
12.	最终问题，怎么从pair到确定对应周期内的交易策略？


现在的问题出在哪？
1.	涨跌停需要额外处理。在我用单日的计算测试的过程中，发现尽管直接剔除涨跌停不会影响HY corr的分子，但是分母仍然是照常相加的！！所以这块应该需要处理，应该是要把对应时间段的数据剔除掉。
2.	跨天究竟应该怎么算？我现在还是感觉，首先如果能够确保单天这个计算的相关性和leadlag是正确的话，那么我应该去不同天按照论文去每天计算然后取平均。如果把相邻两天的数据拼到一起，那么也理应把时间点拼起来，因为这个correlation的unbiasedness和consistency的保证要求就是largest of the mesh grid尽可能的相近，然而隔了一晚上，也就是前一天和后一天的数据没有重合，分子不会计算。但是同样的道理，分母仍然会全部考虑计算，这样就会有问题。所以其实应该是拼起来的，但是第一个是具体怎么拼？是9:30拼到14:57？侯总又说因为晚上会有新的信息，所以两天不应该算到一起？那按照这么说，就是只能每天算然后取平均。当然这一切都需要先保证我写的这个计算单独一天的HY corr是没问题的，这个还要多画图多看行情去验证一下。
3.	究竟是否应该取tick time？取tick time是否会导致画出来的价格走势图和实际的价格走势图是不一致的？也就是算出来是有leadlag但实际上从通达信看出来是没有leadlag的？
4.	为什么时间从两个月减到了一个月，中银证券的效果差别这么大？？这个HY correlation到底能不能用？
5.	计算HY关联的时候的prev_time的问题，处理了涨跌停之后，这个prev_time究竟应该在外面提前算好，然后一起平移，还是仍然可以在函数里面去平移？

**一些反思**
第一个应该是检验单天的HY的计算到底对不对。如果正确并且能用，我才能继续，把涨跌停想办法处理了，然后是写一个每天单独计算的程序去验证一下。

今天测试发现这个东西也并不是完全没用的。目前暂时把涨跌停的改了之后，同时尝试了使用tick time和不使用tick time的效果，目前的发现如下：
1.	上面的那些问题，有些是重要的，有些是不重要的。HY的功能强大之处在于他可以解决流动性不同的问题。试想，一个交易特别频繁，一个不频繁，如果你用传统的插值法，那必然结果是相当不准确的，或者是previous tick，因为你都是需插值，然后大量的都是重复的数值，导致计算指标的时候其实是信息重复的，也就必然会出现更加liquid的那个去lead不那么liquid的那个asset。而HY	解决了这个问题，因为HY的计算方法是算区间覆盖，你无论二者流动性相差多大，不频繁交易的那个asset，他相邻区间的跨度会比较大，这样大区间会和频繁交易的asset里面很多的小区间产生重合，因此不会产生信息流失！所以tick time的数据量少也许不是一个特别严重的问题。但是，涨跌停一定是一个很大的问题。原因就在于，涨跌停的时候，如果我们使用的tick time，那么就会排除掉发生涨跌停的资产的后续数据，然后刚刚发生涨跌停和刚刚解除涨跌停的时候的midquote很大，我们也需要筛除。这样导致的后果就是，我们计算HY相关性使用的区间，对于这个asset X，只会有涨停前的区间，和涨停解除后的区间，而不包含涨跌停和解除涨跌停的对应的端点。这就导致不会有一个区间，跨越X的涨停前和涨停后去和Y去计算交集。然而分母仍然会把Y在这段涨跌停时间内的midquote计算进去！！所以必须要把对应区间的Y的midquote给去掉。不然就会导致分母在增加但是分子并没有参与计算的情况。
2.	那隔夜是否影响呢？我感觉会影响！根据上面的推理，只需要保证不会出现分子分母有一个在一直增加但是另一个没有的情况就行。而隔夜的情况，两个资产仍然保持一起交易，分母也不会出现涨跌停那种凭空增多的情况。但是！会出现第二天早上与头一天最后一笔的一个大跨度的区间，这个区间有影响吗？也许有，也许没，涨跌停由于我对于midquote的大小范围的限制导致不会出现大跨度的区间，但是隔夜的情况一定有，这个隔夜的大区间会和前一天另一样资产的密集数据产生交集，因此这么看是不合理的，因为夜晚是有额外的信息，我们应该是不能让前一天的数据和第二天的产生交集的。但当然这产生交集的毕竟是小部分数据，毕竟收盘后就没有数据也不会产生交集了，感觉问题不是很大，需要处理了可以单独处理一下。这么看最合理的办法还是单独每天算，然后取平均。

**现在需要做的事情**
自己写的版本，把prev_time的位置搞清楚，把隔夜和涨跌停的问题彻底解决了。然后开始部署基于那个内置函数的并行版本的代码，并行的逻辑肯定都是要全部重写的，因为这个时候并行的对象就不是股票对了，当然期待做四千扫四千的时候可以用并行。一个扫四千就直接是矩阵拼接就ok，中途可以自己想想可以怎么加速，numpy，numba永远的神。还要搞清楚关键问题，就是为什么涉及到隔夜计算内置函数和自己写的版本结果不一样。这个必须解决清楚，然后再去说是否应该单独按天算，还是单独上下午算取平均，还是可以合并算，当运行速度大大加快的时候，这些东西都可以开始研究了，包括intraday profile，比如我们只关注早盘的leadlag现象，只用早盘的半个小时等等。这就可以研究了，当然还是得先搞明白代码究竟哪里会不会有问题才行。
改用对数价格重新跑。

其实新版的设计思路已经有了，除了可以对四千×四千的做并行，HY的第一个参数传入的是一个series的list，那这就很方便，简单的map得到的结果就是list，或者直接dask并行或者parallel并行得到的结果本来就也是一个list！那就四千乘四千的时候，对四千个做并行，然后对每一个，处理另外的四千个的数据的时候也可以并行。具体来说，我还是打算把需要的所有股票的数据给并行处理好，然后每次对一个，取另外四千个的数据，另外四千个就并行把server time给平移了（甚至这都不需要并行），然后就一个list传入HY的函数就可以了，感觉应该也不需要，因为就一步操作……

现在问题是似乎需要重写两个版本，一个是天求平均的，一个是一起算的，我TM……这个HY应该需要天求平均！这样可以对日期并行？

**自己更改过的模块记得调出来改到我自己的电脑上……包括binance等源代码，还有transport.py里面的参数，还有HFHD里面的HF全部！！到时候记得发回来……**

还有一个目前非常严重的问题，除了那个HY接口处理不了剔除涨跌停和隔夜情况的大区间交集的问题意外，更严重的是我发现，是不是应该每次计算lag的时候都应该重新剔除一次涨跌停啊？？？我傻了，我现在是对原始数据剔除涨跌停对应的数据之后才去计算lag的，但是这样科学吗？以及我写好了函数但是因为涨跌停的存在不能加速，按理来说大批量加速计算可以实现，可以很快，一只扫三百只可以两分钟就完成，前提是数据提前处理好，现在问题就是每个pair都要去对涨跌停处理，主要问题就在涨跌停，导致我不能把几百个asset放到一个list里面直接计算HY关联。烦死了，这个涨跌停到底怎么弄，我干脆剔除这些天数算了操。

**汇报**
1. 最新的结果。Tick time和非tick time的区别（看visualization），然后每日取均值和跨天算的区别。
2. 涨跌停的问题，我目前对涨跌停怎么处理的，以及对隔夜的怎么处理的，然后究竟应该先剔除再lag还是先lag后剔除涨跌停？
3. 并行程序如何设计的问题。如果要考虑涨跌停就寄了。包括最新的接口的问题。

**数据储存优化**
Redis数据库安装，尝试减少读取数据的时间。Linux的sudo apt update无法使用，无法安装ubuntu的apt命令？是权限的问题吗。目前redis还没有跑通。


#### 2.5 2022.06.27进度总结
目前因为侯总没来公司，整整两周没有汇报，进度停滞，不知道接下来干什么。目前阶段的HY的检验部分基本已经做好了，已经可以看接下来怎么设计策略了，我自己也在试着写一些模型预测，包括试着用babuqtuant的套利程序去试一下能不能搞，但是现在想来估计还是得写预测的程序，因为leadlag产生的套利机会，如果是按照huth的文章的交易策略，那就还是在做单边交易。这样的话leader的收益率序列/价差序列就完全是一个lagger的一个因子，与lagger的历史盘口因子一起参与预测就好了。所以如果侯总再不来，我就只能要么试着把babyquant的框架和之前写的那个预测的HFT project框架结合一下写一个机器学习的预测框架，要么就试着写一个实盘交易下使用HY的框架，就按苑总说的那样累加计算看效果。要么就只能继续多看几篇套利的paper，然后试着做一下了，关于HY的还有一些paper，里面使用的策略可以读读看能不能用，另外就是其他的套利的paper了，读一读正好也算准备下面试。实在不行就只能去找吴哥去要点任务做了。

周二要是侯总再不来就先看paper吧，明天把HY的一篇文章，还有其他几篇经典的套利文章都看了，然后在这里做一下笔记，同时看有没有可以复现代码的部分。

**大概总结下现在HY这部分的问题和成果**

**成果**
- 现在最大的成果就是HY的计算非常优化，在工具包中写入了自己的函数，现在可以计算多个序列之间两两的HY，同时可以计算一个asset扫其他一堆asset的HY，这样方便计算lagged的HY关系。并且整体代码用向量化的速度非常的快，可以输出平均计算的结果，也可以输出对应的标准差。也就是说如果认为每天算取平均可以接受，同时涨跌停就去除，那么这样计算就是很快，甚至在下一步的并行算法进一步优化，或者想办法可以用gpu训练的时候，甚至可以实现四千扫四千。目前尝试的是单纯增加数据的时间长度并不会太加长运行的时间，也就是取很多个交易日取平均，甚至说取半年，都不会让速度变慢太多，主要还是参与计算的资产的数目以及并行的方法。
- 在取平均的时间段足够长，比如一个多月，并且使用的不是tick time的情况下，可以实现比较精准的衡量，基本上榜单前几名都是同板块的（证券是这样，但好像换了银行什么的结果不太是银行？？）总而言之，无论怎么说，肯定是可以代替一部分的之前leadlag的计算，就是单纯使用线性回归和lasso回归做的部分的效果。虽然我也不知道HY这个东西的效果有没有回归做的好，毕竟回归的逻辑又简单又实用，比如就单纯拿某种资产的future return对其他资产的历史return回归，可以分别回归看系数显著性，或者直接跑一个多元lasso，然后看剩余的系数如何，未来一段时间就根据这个系数去交易。我猜大概就是用target的过去一段时间的future return与全市场的return跑lasso，lasso完之后留下显著的asset，一般就是同一个板块的，然后target asset再对这些用lasso选出来的asset分别做一个线性回归，然后用回归的系数做预测，比如用选出来的这些asset的return分别做一个target asset的future return的一个预测，然后对这个预测取平均值作为估计决定是否买入卖出等等。这样感觉是比较科学，但是这种回归一定是要插值的，甚至要用一些EWMA去平滑，**然后跑线性回归，因为线性回归的系数本质就是一个插值后同步的一个correlation**。
- 现在就是说一个本质，用HY最差的情况可以替代掉一部分之前用回归跑的结果，其实HY关系可以说本质上就是替代了一元线性回归的那个coefficient，因为一元线性回归的coefficient就是相关系数，用这个相关系数做leadlag的衡量或者做预测，那就可以把对应的这部分换成HY correlation（当然多元lasso这种筛选的功能就不能只用HY实现了，因为HY只能两两的算，而且是否显著的判断也需要另外独自添加判断条件）。总而言之，就是HY的作用和这个线性回归的作用很相似，lasso回归也可以通过HY和LLR的一些东西替代，但当然1.效果是否会比回归好，以及具体怎么做这个筛选，那完全是新的调参的空间，现在就是需要侯总的指导，这个东西我自己不知道怎么做；2.怎么拿来预测，选定了leader的asset以及对应的权重，那可以考虑用leader的return做预测，用correlation去预测，但是这个预测的方式也是不一样的。比如线性回归你就是用那个系数去预测，要么一个leader一个一元回归然后取平均，要么一块一个多元回归做prediction；但是HY预测就可以使用不同lag time的系数做预测（或者只用significant lag），**这就相当于每一个leader，都在跑一个多元回归的prediction**。最后可以取平均或者怎么用的。
- 再说一点关于LLR目前的理解。LLR本身就是一个衡量bi-direction的leadlag的一个相对强弱指标。他本身的作用并没那么明显，因为我们的目标如果说是去交易一个target asset Y，那么我们就是要找现在在lead Y的这些资产，那可能我们可以只挑选那些lead Y比Y lead他更显著的资产做预测，也可以把所有对Y存在lead的（也就是不管LLR多大）的都用来参与预测。而如果我们考虑的是交易一个对子，比如X lead Y更显著的时候交易Y，而反过来Y lead X的时候交易X这种策略，那LLR是有用的，不过感觉这种应用场景很少。

**总结一下就是HY相比回归的缺点和优点**
- **回归的优点**：回归速度快，原理简单而清晰明确，lasso用来选有用的leader，回归系数用来做prediction。实现起来很快，适合实盘；**回归的缺点**：中间需要插值，并且一元回归的本质就是一个synchronous的linear correlation，缺点类似previous tick，甚至不如previous tick（甚至可以考虑用previous tick替代一元回归的系数），就是算出来的是biased，Inconsistent，并且会收到micro noise的影响。最严重的缺点还是会出现总是比较liquid的那样资产在始终lead不是那个liquid的资产，因为不是那么liquid的资产在计算的时候会不停的插值interpolation，然后回归就会导致这个情况。就算使用EWMA做平滑仍然不会有很好的缓解。当然，如果是A股，并且两个交易频率相近，而且用original time，都是快照数据比如3s的话，那数据的采样频次差不多，影响就不会那么大，毕竟我们tick数据都是快照的，频次接近。但如果使用的是tick time，那流动性的问题就会马上暴露出来。
- **HY的缺点**：计算慢，不知道怎么用（包括不同time lag的系数怎么用来预测收益，取多长的时间计算，多久rolling一次，多久更迭一次，LLR怎么用，怎么筛选leader等等都是需要确定的问题），甚至可以考虑一部分用HY，剩下一部分还用回归，比如使用lasso做筛选，然后用HY的系数去做预测这样因为HY的筛选机制不明确等等，手头有不同lag的HY系数，怎么用，怎么算，怎么做预测，怎么做策略，都不知道。**HY的优点**：最大优点就是数据足够高频下反应的是真实的时序相关性，同时自动处理异步性的问题，以及可以提供很多的信息，就是不同time lag下的一个系数，这些系数往往都可以提供一些信息，**能具体告知lead几秒或者lag几秒肯定是很有用的信息**。只是怎么用，现在不知道罢了。还有一个很显著的优点，就是现在计算HY的时候用的是内置的函数，里面是有一个阈值参数的，这个参数很有用，甚至说对流动性不同的asset会专门有推荐的值，比如0.4-0.6一类的，这个参数显然也可以调整和优化，如果需要用来预测的话这个参数是肯定有用的。



**问题**
- 部署的并行程序还需要优化，比如试过是不太能一个parallel里面再套一个parallel的。这样反而核数都没用起来。在真正需要几千扫几千的时候需要根据实际情况再写并行程序。包括计算股票数据的时候，可能还需要根据date和lag的相对的一个数目大小决定对哪个并行，然后只能对另一个用迭代器，**至少目前没有想到一个更好的办法，能让他们全部并行起来的办法**。而且之前跑股票数据的时候对日期并行好像一直跑不了。
- 涨跌停和隔夜的问题。涨跌停：是否可以直接剔除，不处理；如果处理的话，需要每一个lag都处理，也就是之前的代码还是有问题的，不能先处理好然后分别计算lag，这样大的时间跨度仍然是会cover的，要么就选择直接不处理，把对应的天数扔掉，要么就选择忽略这个大的区间跨度。隔夜：我自己写的代码是可以处理隔夜的问题的，就算计入了lag应该也是不会产生大的区间的，但是如果要使用built-in的HY方法那就不行，那种方法只能算连续时间的日内的，因为需要预先把时间输入进去，然后他去做匹配，在内部去做shift。总而言之从各种时间精力的角度考虑不如就直接计算日内取平均。
- 日内算取平均，整体算，以及上下午分开算的结果还是很不一样的，不过时间足够长的话结果是相似的。但是使用tick time和original time的结果是完全不一样的，甚至原先lead的在tick time下直接就是lag的榜首，这个还是很不一样的，所以需要看到底要用哪个，哪个才能拿来用来做套利。目前看的文章里面都是在用tick time，以及对应的这个correlation做预测，但tick time跑出来的结果就是证券板块的前几名就不太会是证券，当然时间更长的没试过，一两个月的时间前几名不是证券就已经比较奇怪了。

#### 2.6 一些完善性的研究
**全A股聚类研究**
仿照sklearn的那个similarity measure的tut，使用各种聚类方法，包括熟悉一下类似多元高斯分布correlation matrix估计的图lasso方法，练习一下机器学习。

**Granger因果检验**
statsmodels实现。仿照HF包里面获取上三角matrix的index的方法去计算并获取p检验值，statsmodels的检验是需要pairwise进行，然后检验的是dataframe的第二列对第一列产生granger因果的显著性，所以implement的时候需要pairwise并行并且注意列排列的顺序。

**总结下similarity measure与granger test的必要性**
本打算花更多的时间尝试对数据填充匹配对齐，然后尝试严谨的cluster，做similarity衡量，然后做granger检验，然后做一做lasso回归的预测。现在想不如更多的时间花来做模型来赶紧学点深度学习为下一段实习蓄力。也算是把策略回测前的工作做的尽可能的完善了。总结一下:

在我现在看来cluster和一些常见的similarity measure仅仅是初步的一些分析,如果数据的时间线长一些或者使用的日线数据,那么基本上cluster是能反应很多有意思的信息.对于correlation matrix的衡量现在有太多的手段,比如线性系数/HY系数矩阵/对高斯序列用的图lasso等拟合技术/dynamic time warping等各种方法.正好有affinity propagation这种方法可以测试这些correlation matrix的准确性.当然,这么多的方法衡量两个时序的相关性,很有用,能给我们关于原始时间序列的基本认知,但是都仅限于原序列.我们的最终目的是预测,这些工具只能近似告诉我们原始序列或者说偏宏观上的相关性.所以HY相关系数或者lasso regression的作用仍然是无法替代的.当然在用这两种工具做leadlag的预测的时候,提前使用granger因果检验是必要的,granger也是基于原始序列,并且不能给我们有关预测的帮助,但是granger能告诉我们究竟能不能用X来预测Y.如果可以,如果能,那我们再去用lasso回归,再去用HY相关系数做预测.毕竟本身HY系数大小和LLR这种东西不能告诉我们序列X对Y的预测效果究竟是怎样,因为缺乏一个严谨的statistical test.而这就是granger test的意义所在.


#### 2.7 基于Leadlag的统计套利策略本质

- Leadlag策略的本质是统计套利。统计套利说的是我们利用现价和fair price的差值进行套利，统计是用来计算fair price的统计模型。而leadlag策略本质就是利用股票对的信息差距，我们用快的那个去预测慢的那个资产在此时此刻的fair price，然后利用现价和fair price地方差距进行套利，我们用的预测lagger的fair price的模型就是使用leader的一个线性回归模型（也可以是其他模型），然后等到ticker时间到，lagger的价格回复到我们的fair price，我们就可以获利，所以leadlag策略本质就是统计套利。

- **Leadlag是最能解释为什么统计套利的本质是做趋势。我们始终在用leader去预测lagger，交易也只是交易的lagger，决定了pair之后的任务就是对lagger做单边的趋势，就是在做日内的CTA，就是在做趋势预测，做择时。**

- <b><font color=blue>Leadlag策略在赚什么钱？在套什么利？<font color=red>做leadlag策略交易的对象一定是价格走势在低频上高度一致的但是高频上的短期内并不一致的对象</font>，因为根据epps effect，高度相关的两样资产在微观上的相关性会降低。数据切片频率越高，相关系数越低。因此我们交易这种价格走势上宏观一致的但短期不一致的，那么在高频时点二者对信息的反应并不能达到同步，相关性会比低频数据算来的要低，此时就会产生lead lag，然后在我们预测做出反应的时候，随着时间拉长，两者的价格走势趋同，序列继续维持一致，回到fair price，我们就可以套利，这就是leadlag在赚的钱。</font></b>


#### 2.8 如何构建预测策略

**文章的套利策略以及可能的优化**
- 具体实现就是先滚动的回测，滚动的选lead-lag pair，然后去交易，pair的选取/调仓好比选股策略，比如每周一调，不过一般应该还是每日平仓调仓，每日更新pair，甚至可以像前面说的更加的高频，就是每日考虑intraday的LLR和correlation的profile，比如根据不同时段早收盘前，中午开盘后，盘中分别调仓使用不同的pair，那么相应的回测选取pair也应该是前若干天滚动的对应时段的数据计算的correlation。

- 具体怎么选pair，是一个大难题。知道要考虑LLR，但更要考虑本身的correlation要高，同时可能也需要考虑最大的lead correlation和最大的lag correlation，具体怎么选取，文献里只给出了network的用法以及一个很模糊的说辞，只是说都要考虑，具体怎么用还得自己开发研究。

- 选好pair之后在对应的交易期间内（比如一天内、不同时段内），要做的就是趋势策略的搭建。最简单的根据文章中的，使用leader的return跑一个线性模型去预测下一个tick time的lagger的return的涨跌，做一个二分类，回归预测的return大于零就涨，小于零就跌。然后每次预测结果出来都去交易（因为做的不是回归不知道具体涨多少跌多少），按市价单去开多单空单（即在ask1去买在bid1去卖）。当然这里能优化的太多了，一个是预测的因子太少，预测的模型太简单，而且开仓平仓的时点也很粗糙，凡是预测完就开仓，凡是到预测的下一个tick time就平仓。

- 如同文章所说，我们计算correlation和预测未必使用tick time，可以是多个tick time。预测涨跌或者预测具体的变化都可以，预测涨跌就是文章里面那个二分类问题，只关心符号正负（文章面对分类问题仍然做的是回归，因为只要回归值大于0就是涨小于0就是跌）；预测具体midquote的变化就是回归问题，而我们可以设置比如预测midquote change大于一定的阈值我们再开多和开空，否则不动。**根据预测midquote变化超过多少阈值再开多开空也就是调整开仓的时点，然后根据预测的是多少个tick time后的收益率去决定平仓的时间点**。总之就是开仓的时点、平仓的时点、HY correlation计算的时点都可以考虑加上阈值的版本。
  
- 止盈止损，这里就不说了。
- HY系数本身带有一个threshold参数，这个参数显然可以用上。
- 如果只使用leader的收益率序列做线性的预测，文章里是用所有显著的time lag，然后对不同的time lag的leader的return的跑一个线性预测。那么预测使用的系数是什么？文章中应该是过去20天的均值，我个人认为要么就用前几天的，要么就滚动的算，像那个实盘的框架一样直接滚动的计算这个系数，这显然也是一种优化，而且贼方便的是不用rolling，直接做一个动态增加的框架然后实时的更新系数即可，**相当于整个HY不同lag的系数和LLR都在一个动态更新的过程**。然后如果用的是tick time，就每次预测到下一个tick的收益率后，比如如果收益率绝对值超过了手续费，那就开多或者开空，然后在预测的下一个tick的时点平仓。如果使用的是original的快照时间，那么可以考虑每个快照都预测下一个快照，然后超过阈值开仓，然后到下一个快照就平仓。当然这样频率太高，也可以考虑像原先paper说的一样对原始数据点做一个采样（这个采样也是babyquant经常做的事情），然后预测下一个采样点的收益率，这样的开仓平仓的时间间隔就稍微长一些，不至于交易太过频繁，或者就像babyquant一样设置什么开仓平仓阈值，阈值高一点自然交易的次数就少一点，**比如我们认为预测的收益率超过两倍的spread才开仓等等**。**再详细一点，假设X是leader，Y是lagger，那么是每次Y到了一个行情的时候我们才去立刻做prediction还是根据X的节奏每次X来一个行情的时候做prediction？按理这个过程没用到lagger的任何信息，所以理论上是有了X的行情就可以立刻更新对Y的预测，所以理论是X产生了行情就预测，然后预测有信号了就下单？是这样吗？**

**第二种套利策略**
- 在另一篇HY的衍生paper中给了另一种的交易策略。把每个连续的X行情和Y行情放到一个cluster里面，X和Y就是midquote价格，可以用对数价格但文章就是价格，那么每个价格时间点都可以计算对应时点相对于上一个时点的差值，就是价差，**当一个cluster内存在一个收益率或者对数收益率的绝对值大于一定阈值的时候，产生交易信号（1代表开多，-1代表开空，没超过阈值就算0就是不开仓），而产生交易信号后立刻开Y的仓位（这样的话和上一种方法都是在X出现行情的时候判断是否有信号，有信号就立刻开仓），然后一直持有到一整个Y的cluster，直到X下次行情出现就平仓**。这种方法相比与前一种，操作性更强，逻辑比较简单，实现起来应该也更简单，PNL很好计算！然而可能也有一些问题，比如根本就没有用到不同time lag的correlation的信息，**而是完全以cluster作为单位，根据leader的涨跌来预测是否产生交易信号，相当于默认X lead Y就是X在领先Y一个cluster这个样子**。**统计accuracy的时候也是判断X的cluster整体的价差是否和下一个紧接的Y的cluster的整体价差同号**。那如果leader的lead corr并不是很显著，这样显然就会出现一些误判的情况。所以本身这也属于一种naive strategy。感觉甚至太过naive，因为完全就相当于选择了pair之后就完全没有用到任何历史的HY corr的信息……这里能调的就只有开仓的对leader设置的阈值K，以及每次开仓的合约数量。
- 对于第一个参数，每次有信号开仓的数量，开仓的数量可以是固定手数，但同样可以根据预测值绝对值的大小，第二种策略就是根据X的cluster内的收益率的绝对值大小来决定开仓的仓位。这也是一个优化空间。第二篇文章提到了一次信号只交易一张合约的话不用考虑market impact，倒也是。如果增加单次交易合约数目，手续费可能会吃得少，因为手续费可能不是按合约张数扣的，而是按order扣的。但是market impact也会变大，所以就是5530学的一个trade-off，不能太大不能太小。
- 对于第二个参数，阈值K，类似这种开仓阈值或平仓阈值，就应该像babyquant一样去做网格优化。这里不涉及平仓阈值，就一个开仓阈值，显然很好做优化，使用不同的(lead, lag)交易对的最佳K不一样，K太小，很吃手续费，K太大，交易太不频繁，降低收益。所以又是一个trade-off。同时还发现了K太大还有另一个风险，每次predict的return或者X的return比较大的时候，对应的reaction time会大幅下降，也就是交易机会更加转瞬即逝。
- **这种交易方法虽然简单愚蠢，但是优点就是计算极快，因为交易信号的产生只需要做一次Logical comparison，运算极快，也正好比较适合超高频交易**，就像第二篇文章里面这种，做的就是跨国际期货交易所套ETF这种需要超高频的数据，超高频的信号的交易。的确如果是这种超高频的数据和信号需求，就算是tick time，我们前面那种计算HY的方法也吃不消，哪怕不是滚动而是从开盘开始累加，也需要计算interval是否overlap，大概是三次的for loop，支持不起毫秒级的计算需求。



**趋势策略**
- 可以使用更多的特征，我们一般不会只使用**leader的midquote change来做预测对吧，说的简单些，leader的midquote change序列，就相当于我们挖掘的针对于lagger交易的一个重要因子**。仅使用leader的序列预测lagger，相当于我们在做一个单因子的回测，做一个单因子的趋势CTA。那我们显然可以把这个单因子和其他的因子合成起来。leader本身就有很多，显然可以都用上。或者比如在不知道这个lead-lag的时候，我们肯定会用这个lagger本身的各种历史价量信息，各种技术指标，还有各种比如订单簿因子成交因子委托因子去做时序预测，那我们就可以把这些因子合成起来，然后给leader的return序列加一个高的权重，做一些什么标准化啊，预处理，包括因子正交，还可以进一步做遗传规划因子挖掘，然后去跑回测。而且我们当然也不会只用leader的return这个单因子，可以用这个字段去生成price序列有关的各种因子，也就是leader return序列的技术指标因子、depth ratio，各种什么偏度因子、峰度因子，包括babyquant使用的各种EWMA，双均线合成因子、波动率因子等等。同样也可以进一步使用统计函数做leader因子挖掘，还可以leader和lagger的数据结合在一起挖掘，思路很多很开阔了。

- 可以使用非线性模型。尽管LLR本身的推导是依赖于线性模型的，但是LLR的作用只是用来选pair，而并不是用来做预测做交易。他只是告诉我们leader的return序列很有用。我们完全可以拿来，结合上面的那些因子，去喂到各种机器学习模型里面。其实到了这一步，基本上就和babyquant的CTA是一致的了，因为babyquant的CTA就是在做单品种的交易与回测，而leadlag本身就是在做单边的趋势策略啊！那就是完全一样，可以按照babyquant的方式去生成上面说的那些因子，然后还可以做因子合成，就是方向因子乘上趋势因子，然后喂到各种机器学习模型，线性回归、lasso、ridge、elasticnet、adaboost、GBDT、xgboost、lightgbm、RNN、CNN、MLP等等……做出预测然后交易。这本身可以是一个分类问题也可以是一个回归问题，反正预测的是下一个tick time的Midquote的涨跌，二分类问题或者回归问题，那就是机器学习的天堂。


#### 2.8 2022.07.06下一步的规划
目前leadlag进度停滞，需要进一步的指导去开发日内的趋势预测，主要是完全没有这方面的经验，必须有leader指导啊。再加上也同时段在看套利的文章，也在学期货的CTA，现在是7月6号，突然感觉紧迫了起来，实习已经过去快两个月了……如果真的要考虑月底新申请或者8月换实习，我得保证这段实习有充足的经历，学够了东西，做够了充足的project才行啊。现在实打实过去了快两个月了，能拿得出手的只有一个project，而且还没有完成最后的策略设计。现在必须开始新的project了，毕竟上一个project已经做了我力所能及的一切了，需要指导才能往下做。我打算把重心放到做模型上面了。本来我来这家公司，想做深度学习时序预测，本来就是做模型，想着借这个机会把之前学的各类ML和DL方法投入实践，但是没想到给我说没卡。但没卡也不影响我做模型，而且后面找实习一定很看重个人机器学习和深度学习的能力，我想趁这段正好有时间，自己把学习重心放到ML和DL上面，包括各类time series model，LSTM，transformer，CNN，图神经网络GNN等等，因为我大四应该也不会再花时间上FTE4560这种课了，所以接下来的机器学习进阶和深度学习就要全靠我自己自学了，我会在大四下选上强化学习DDA4230，然后final project做一个强化学习的交易系统，这方面的project在github肯定是很多的，花一门课的功夫把强化学习的经典模型都学了，包括机器之心SOTA里面的各类RL模型。总之DL接下来的部分是要我自学了，图神经网络和语言模型这些东西，NLP，估计要等到硕士才能上到课，所以只能自学。那就把重心全部放在ML、DL的学习上吧！希望到大四下结束我能MLDLRL三修，实习一份接一份。
这周前两天借机会把statsmodels和cluster复习了一下，现在ML的部分感觉还算好，打算主攻深度学习。目前有三个想做的事情：
1. 先复习好以前自己写的MLDL模型的slides和code，然后去gitub把pytorch的tutorial刷一遍，然后做好笔记，再去做接下来的部分
2. 自己开发套利策略，就复现文章中的那个多空预测框架，文章有代码，里面也有模型预测的部分，就在这部分开发自己的DL预测
3. 自己开发CTA策略，把期货的CTA用到股市上，按照期货CTA的流程走一遍，接这个机会正好把之前没理解的各种框架问题都理解清楚

以上相当于套利和CTA两个方向开发的大project了，希望我能努力做完。另外更希望的是把以前学过但从没用过的东西全部复习整合起来，包括HFT的那个project，Kaggle的optiver大赛，里面用到的方法架构完全可以在模型这一块应用起来，开发更多的因子和特征。还有以前整理的DL的tut，做过的project，现在都捡起来，看学术论文，然后去github找代码，复现，优化改进。

当然，以上三个部分都是在闲下来的时候去做的，侯总要是在指导那还是按他的要求去做，我还是希望从他身上学到更多东西的，包括套一些策略思路和研究思路等等，毕竟光靠我一个人摸进步太慢，拿不到推荐信，下一段实习也很难找。还是要主动一点，既然回来了就主动去请教。最近变故有点多，侯总生病要治疗一个月，这边吴哥又打算走了，也许这是个机会，因为之前一直说我没有卡来训练，等吴哥走了之后是不是我就可以用卡了？但是我现在实在深度学习的水平太拉跨，就算给我卡也不会做，不如我打算就最近好好准备gre，然后好好把深度学习transformer一类的东西做了，等回来了也许我就有机会做模型了。无论如何，希望度过这个艰难的时候吧。

---
<div align=center>

### QuantLab、Sunnytrader
</div>


**回测系统介绍**
sunnytrader是由黄灿老板开发的交易系统，quantlab是由启能达和盛冠达IT部门合作开发的平台，在sunnytrader的基础上进一步的开发和迭代成完善的回测系统，目标是对标卡方面向全市场，提供类似的金融回测服务平台，做成产品化。底层接口是C#编写，回测速度很高，同时可以自己调节策略参数细节，提供每日交易的详细持仓，实时反馈的可视化曲线，各类评价指标，还有对回测结果的归因处理。总体功能完善，并且可以直接对接实盘下单。目前盛冠达的IT和量化开发做的就是这些东西，quantlab会不停的迭代，而且似乎还每隔一个月就要更新，整体版本不断升级迭代，去年到今年就已经从quantlab2升级到了quantlab3，功能越来越完善，目前做的最好的交易算法，TWAP和VWAP已经非常完善，还有T0算法（T0交易下单是盛冠达的招牌）。建议仔细研读文档，研读样例，尤其是交易下单T0的部分，这是sgd的头牌算法，同时在`sgd_share`里面由T0 project，宝藏这么多，一定要好好学啊，顺便温习下C++和C#，这样过后就指导量化开发工程师是在干什么的了。



#### 数据系统

**回测平台架构**
目前实现的更新：包括升级到.NET5.0和sunnytrader同步，实现跨语言跨平台等等（原先只支持windows下的C#）
- Level 1 基于文件的数据系统（跨平台，跨语言）
- Level 2 (.NET 5.0) 数据API
- Level 3 (.NET 5.0) QUANTLAB

**数据系统**
- 数据类别：
  - 股票：日线、分钟线、Tick级、逐笔成交、逐笔委托
  - 期货：日线、分钟线、Tick级(L1&L2)
- 数据路径：Windows共享: `\\192.168.1.147\sgd-data\data`；Linux NFS: `sudo mount 192.168.1.147:/sgd-data/sgd-data`；用户名：traders；密码：abcd4321，或者另一组用户名sgd，密码sgd123。后面的这组用户名和密码可以访问更多的权限，前面的账户密码只能接触到147服务器的data文件夹，而后面的可以接触其他的文件夹，包括`sgd_share`文件夹，里面应该是有成熟的策略，包括一个corr还有一个T0 strategy的东西，值得仔细研读。数据格式两种，`csv.gz`格式是适合pandas，因为pandas读取csv较快，而csv.gz对csv文件实现进一步的压缩，读取的时候可以使用`pd.read_csv`读取。`pb.gz`格式适合C++或者C#等，protocol buffer格式。
- windows连接公开服务器的方法（注意必须公开，139就不行），直接在文件explorer的根目录，上面输入两个斜杠，`\\192.168.1.147`，输入密码即可访问。每次重启需要重新输入一次，接下来就不用。直接用文件管理器打开打开的是共享的部分，可以看到只能查看sgd_data, sgd_share等文件夹。而如果使用vscode做远程连接，open folder里面退出/home目录，就可以看到这些sgd_data, sgd_share的文件夹，还有其他的一些没有公开的目录。或者直接win+R，然后在弹出框里面输入`\\192.168.1.147`，效果是一样的。
- 目前sgd_data下面的data下面是，future是期货基础数据，future_l2是期货的level2数据，然后有股票的数据，成交数据`stock_order`，还有逐笔数据`stock_trans`，再点开里面，数据是按照每天一个文件夹存储的，里面有`candle_csv/candle_pb`分钟线数据，`daily_csv/pb`日线数据，`tick_csv/pb`是tick数据，每一个文件夹下面是全体A股的`.csv.gz`或者`.pb.gz`的合集。其他的DataGen文件我盲猜是让这个服务器能实时获取数据并进行记录的工具。
- 毫无疑问我们要访问147服务器需要连接公司的网络。我们在`RemoteQuery`里面是从147把数据拿到了139里面放到了一个缓存的路径，目的是减少网络通讯。如果不在我们指定的缓存路径`backtest_temp`下，我们就会直接读取。直接读取csv.gz和直接读csv的方式一模一样，唯一不同就是服务器路径前面是两个斜杠，即`data = pd.read_csv(r"\\192.168.1.147\sgd-data\data\stock\20120104\tick_csv\000001.SSE.csv.gz")`即可。csv.gz极其好用，能把原来的csv压缩到接近十分之一的大小，比较好用的就是格式就是csv.gz, parquet, pkl, h5等。

- 要存储的是一整个公司都要使用的数据系统，经过各种测试，这种系统的存储方式的速度是最快的。我们试过各种各样的database，但是要大量读取这种，一整天一整天的tick数据，一定是系统最快。第二是方便管理，非常清晰的可视化，如果是数据库就不方便查看。黄总直接质疑数据是哪的，说是从数据库转换来的，数据库底层是购买的吗？不是，是我们自己录的，黄总直接说经常反应这个录的数据交易质量不高。期货数据下午收盘录入一次，夜盘结束录一次。股票当天的数据四点多会录入，五六点就会录入数据库。

- 目前数据开盘前的集合竞价是有的，还有些有下午三点过后的数据，上午收盘和下午收盘的K线可能都会多那么一两根，因为交易所，比如收盘后会比较数据，会再推送一点数据，自己做策略的时候上午收盘和下午收盘做一个过滤。


#### 回测系统

**环境配置**
- 安装vs2019或者vs2022（vs2022不用配置一些可执行文件），安装.NET CORE 5.0，然后尽量把quantlab解压到D盘根目录下，防止后续调整路径。回测系统的文件夹下可以解压系统讲解的PPT，doc文档，还有压缩包，解压如果需要密码就使用sgd和sgd123。

**策略编写**
三个步骤：建立项目、添加引用、配置调试
- 建立项目：打开visual studio，语言选择C#，选择下面的类库，然后项目命名，文件夹位置放在`MyStrategy`的统一目录下，仍然建议`MyStrategy`设置为根目录，然后选择把解决方案和项目放在同一目录下(也就是place solution and project in the same directory)，之前我没有勾选过这个选项，不勾选就会让`.sln`文件的目录比和项目同名的project文件夹高一级目录，这样方便查找到solution文件打开project，如果勾选的话，solution文件会放到和project一个目录下，由于里面有很多其他的文件，包括主程序cs文件，还有`.csproj`文件等等，虽然乱，但是这样的好处是能让我们的不同策略集中到一个大的父目录下面，方便管理。接下来选择.NET5框架创建。
- 添加引用：右键项目，然后找到添加，然后找到项目引用。然后点击浏览，就可以直接拖拽需要的六个dll文件。


**sunnytrader界面**
- 多线程还是多进程？python只有多进程，因为python强制了不同进程只能在一个线程里面实行。C#，C++这些是多线程，运行界面的指定数目就是多线程，C#多线程效率极高，不像python是需要去设置的。



**tick策略编写**

- 日内tick策略收盘一定会平仓，所以sunnytrader上面的每日的持仓明细是空的，但每日的成交明细是很多条数据的。股票T0策略也是在持有底仓的情况下开多开空，那么当天结束的时候需要保持原来的底仓不变，所以本质都是一样的。



- IT开发了tick策略的两个数学工具，计算移动平均和移动方差的，MovingAverage和MovingDeviation，原因就是用的最频繁，而且如果是python，每个tick过来都要接收行情计算这些指标的话，速度会很慢，rolling极其慢，原因就是每个行情过来都要重新计算，基本和一个for循环没区别。pandas已经尽可能的优化了，里面的ewma等函数都是调用的C的接口。而使用C#、C++实盘的最大原因，一个是下单的速度得到保证，另一个最重要的就是接收到行情推送的时候的因子计算可以非常迅速。之前在CTA的框架里面涉及到大量的滑动平均的因子计算，这里回测框架的实现，一方面利用了C#的速度，另一方面就是根据每日的数据实时推送的机制，去构造了一个队列，每次行情push过来就推进队列，FIFO机制实现，这样计算一些移动指标就极其之快。**这种行情推送的机制才是真正回测的核心，确保没有使用未来数据，这样的实时推送回测也是与实盘保持一致的最佳方式**。之前的那种HY互相关衡量leadlag的公式，本身也可以写成推送的形式，根据每时每刻的推送，去更新迭代互相关函数的分子分母，这就实现了实盘和回测一致的部署。**这样的循环队列可以有效避免重复计算，使得滑动相关的因子计算达到O(1)的复杂度**。


**日线策略**
- 订阅分钟线数据的时候，原始数据只有1min的，订阅其他任意分钟的K线都会由程序自动合成。min的K线数据是通过tick数据合成的，但是黄总说会有一个问题，就是会不准，比如不小心下错单，把价格打的很高，然后又掉下来了，那这个时候tick的行情是拿不到这个信息的，所以使用tick的数据合成会失去这部分信息，然而那分钟的最高价会很高。有需要的话需要自己录一个分钟数据，而不是从tick合成。有个人一直问为什么没有pre_close字段，笑死，分钟线根本不存在复权这个问题啊，分钟线就是直接get过来的，日线的pre_close字段是做过复权的。


**价格撮合机制**
- 系统默认是你指定什么价格马上按照对应价格成交。可以自己定义撮合，股票tick目前可以实现精细的盘口撮合，但多数策略是不需要的，自己指定一个滑点就行了，尤其是中低频的策略。整体大家需求不一样，而且很复杂，索性让大家自己定义。
- 盘口的撮合，已经是足够的精确的回测机制了，我们能够说你设计T0策略尽量保证做好拆单算法，不要有太多的盘口延迟，这一块需要策略本身有一定的robustness稳健性存在。我们只能说在你每回下单的时候根据盘口的具体情况去回测撮合，去根据卖价盘口下单，但是更精确来讲，你的这次下单会影响当前的盘口，这在我们实盘的时候继续推送盘口数据可以捕捉到这一点，但是在回测的时候是不行的，也就是回测的时候我们默认你这个tick的下单对盘口的影响忽略，也就是不会影响到下一个tick，否则你每次下单后续的盘口回测tick数据全都要更改，这个太复杂了。至于其他的像涨跌停不能买这种，那就是策略层面的事情了。


**提高性能的建议**
- OnDataArrive是在每次tick数据推送的时候进行的操作，因此要极力避免耗时的操作以及重复的操作，比如读取文件、读取数据库、访问网络、尤其要避免for循环。读取数据尽量在beforeTrading就读进来，循环尽量避免，访问网络会造成额外的网络通讯。高频策略对每分每秒的算法都要求极致的快速。
- 自己改进策略的算法复杂度，比如均线算法，尽可能的向量化
- 经常使用的需要读入内存的数据尽量在Init()或者BeforeTrading()中完成预处理
- 能使用K线回测就不要使用tick回测
- 能使用DataUtils读取各种数据就不要在策略里面访问147的数据库，因为会造成额外的网络通讯。建议使用`DataUtils.GetTradingDay()`等函数
- 大的数据结构使用class，不要使用struct，比较耗费时间




---

<div align=center>

### 深度学习与时序预测
</div>


接下来设计套利和做预测模型都需要用到大量的深度学习工具，现在是2022.07.07，接下来个把月把重心放在深度学习上，包括从DNN，CNN，RNN，LSTM，GAN，AE，GNN，Bert，GPT，attention，transformer等工具。之后一年不会再上深度学习的课了，但是实习面试的时候却是重中之重，所以从现在开始趁有时间抓紧自学，开学之后的时间就少之又少了。未来一段时间的重心就是深度学习和时序预测了，也算是回归这段实习本该做的事情的正轨了。借这个机会正式入门一下NLP，好好学学language models，上一些网课。一直到大四下申请前重心都可以在这个上面，深度学习，时序预测，然后希望大四下重心可以在强化学习和算法交易两个部分。目前正好有学长一起学，可以相互交流，不会的还能问纽大的学长，还有在百度做过算法的学长。总体而言，股票T0这块，还有CTA的部分，只要是高频，就需要深度学习工具，趁着现在有CPU有GPU的支持，抓紧学习然后投入使用。

### 1. 深度学习理论进阶

#### 1.0 CNN
CNN非常有必要单独列出来说。在股票高频，盘口信息提取这方面，目前为止，我绝对相信CNN的力量。alphanet之前使用CNN据说是根据日线数据，然后通过遗传规划的方式挖掘出若干的因子，然后通过计算因子矩阵的协方差矩阵产生因子关联，然后把这些协方差矩阵输入到CNN里面去提取。有必要极其深入的理解CNN的kernel的作用。covariance matrix是一个二维的表格，这种表格作为input，使用filter去提取矩阵信息。然后四千多只股票就是四千多个协方差矩阵，作为四千多个channel去计算，filter就是对四千多个channel计算卷积然后加bias求和。通过对四千多只股票的涨跌的预测，然后做一个截面多空，截面中性的策略。做多预测涨的概率高的，做空预测跌的概率高的股票。这个思路非常好，计算了因子之间的interaction，然后通过CNN进一步提取信号。



#### 1.1 RNN与LSTM
LSTM自认为是彻底理解了，之前花了时间看课件里面关于LSTM的各种gate的描述，计算公式和反向传播的逻辑，基本能理解了。具体的进一步熟练在于代码的实现，比如之前就讨论发现torch和keras在dropout的实现不一样。

**tensorflow和torch对于LSTM的dropout的不同机制**
查阅官方文档，二者都是默认把dropout的机制看成一种layer，也就是作为一个dropout layer出现。我们知道dropout存在的作用是在一轮的正反向传播当中去mask掉一部分的神经元，但显然程序无法做到这一点，所以做成一个dropout layer，意思就是计算的时候还是全部都算，但是对输出给做一个dropout，就是mask掉的神经元对应的output设置为0。这在torch和tf都是这样，也就是一个sequential的形式，torch的接口更加高级一点，就是能实现之间在nn.LSTM()里面设置多层以及对应的dropout，而tensorflow只能一层一层的叠加实现多hidden layer的LSTM。但是nn.LSTM()里面定义dropout的话所有hidden layers用的dropout概率是一样的，不能自己设置。同时最大的一个区别是LSTM不会在最后一层后面加dropout，也就是如果是单层的LSTM，那么指定dropout将无效，torch不会对单层的时序输出做dropout。所以要实现和tf一样，最后一层也加一个dropout的话，需要手动对torch的LSTM的最后一层的输出做一个dropout，torch有函数可以直接nn.Dropout()即可。
```
import torch
import torch.nn as nn
m = nn.Dropout(p=0.2)
input = torch.randn(20, 16)
output = m(input)
print(output[output==0].shape)
```



#### 1.2 Attention，Transformer


#### 1.3 Bert, GPT  


#### 1.4 GNN





### 2. 深度学习用于时序预测的文献阅读


#### 2.1 Paper: A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction (DA-RNN)






#### 2.2 Paper: Informer (AAAI'21 Best Paper)


#### 2.3 Paper: Autoformer (NeurIPS 2021)



---

<div align=center>

### 统计套利策略研究

</div>

### 1. 经典文献阅读

#### 1.1 前言
这个章节跳出leadlag的框架，回归一下统计套利的大方向、本质理解以及各种不同的玩法，从基础的pair trading晋升到更加复杂的深度学习对价差时序建模等，彻底了解套利的主流玩法，便于对leadlag策略产生一些启发，同时想办法implement到币圈去，因为币圈我之后实盘的主要策略方向还会是套利，无论说期现套利，或者各种**中性策略**本身也属于对冲套利，都会是未来一年实盘的重要方向。

同时借这个机会把ML、DL的工具用起来，gpu的pytorch已经下好，虽然CUDNN还没有安装，一个是因为Ubuntu的20.04的版本在NVIDIA里面只找到了对CUDA的11.4有安装，但是linux服务器预装好的CUDA是10.2版本的……而且下载需要使用sudo命令，我没有被root添加使用权限，所以就没有安装。但现在是可以用显卡训练模型了，没有CUDNN也可以用GPU，但是CUDNN的主要作用就是加速训练，不安装的话可能速度会跟不上。这个问题之后再说，比如给我添加一个sudo权限然后下载一下CUDNN。总之尽快跑起来。

#### 1.2 对统计套利的理解
<b> <font color=blue> 统计套利是一种基于相似私产阶段性的价格偏离获取收益的策略，通常分为资产配对、价差时序建模、组合构建三个部分。</font> </b> 套利和对冲/中性策略始终是息息相关的。中性就是利用负相关性对冲风险，从而目标是收获尽可能无风险的收益，本质目标就还是一个无风险的套利。

- 最基础的套利方式是配对交易，考虑各种pairs，做趋势，二者出现价差，然后价差在一定的轨道范围之外，比如假设正态然后在两个标准差外就多空操作。这都是默认二者同频变化，即使不同频也默认信息差会及时得到弥补。
领先滞后关系leadlag也是一种统计套利。当我们去关心一对pairs中谁更先引领行情的发展时，我们会发现A,B中总是A先动，B后动，这种情况下，我们也许不必要交易价差，而是通过A的信息来预测B，交易B。这个时候本来研究的pairs就变成了做单边交易，A和B的pair trading就变成了B的趋势预测，然后交易B。
**统计套利的本质：统计指的是使用的计算fair price的统计模型，套利是使用现价和fair price的差距进行套利**。价格变化当然是随机的事情，模型也有失效的风险，但都是随机的，我们只要相信自己的模型，去测试和backtest就行了。
但是leadlag本质同样也属于统计套利，因为根据定义，leadlag也是在利用pair之间的信息，通过统计模型（就是leader的信息建模），计算lagger的fair price，然后利用差值进行套利。
但是**为什么说套利的本质是单边做趋势**？需要好好想想。
- 币圈里面，非常经典的套利玩法就是跨交易所套利、跨期套利和期现套利。合约和现货整体走势一致，但是独立盘口，独立盘口就会存在价差，也就是基差。那这个时候都在拿什么股指期货做对冲，那为什么不去研究币对和他们的合约之间的lead lag呢？我觉得在币圈这种lead-lag应该是比较显著的。衍生品种类多，而且玩法多，很容易存在lead-lag，关键就是怎么去制定交易策略了。
- 其他的名词，比如跨品种套利一般说的是不同商品期货品种之间的套利，而跨市场套利那就是不同交易所的套利。
- 当然，我们接触最多的套利，可能还是在构建若干portfolio，然后利用portfolio之间构建**多空组合**，然后检验超额收益的显著性，那其实这个过程就是一个套利的实验，因为多空组合就相当于我们初始没有钱，但是通过多空有钱了。我们之前做单因子检验的时候，最简单的就是截面划分portfolio，然后持有看收益画曲线，而更严谨的就是像IVOL因子检验那样，我们通过对fama三因子回归取残差计算IVOL，然后排序，然后根据不同股票对IVOL的因子暴露值不同，取设计权重构建多空组合，检验超额收益的显著性，那这本质就是套利，因为根据套利的定义，我们认为IVOL小的组合收益高，IVOL大的组合收益低，那么意味着当前IVOL小的组合的价格是被低估的，也就是低于其fair price，IVOL大的组合的价格是被高估的，也就是高于其fair price，所以我们就做多IVOL小的，做空IVOL大的赚取收益（稍微有点牵强，不过突然发现IVOL这个计算的回归过程就很像一个和常见风格因子正交的过程，剔除了市值、账面市值比等因子的贡献部分）。当然美股可以多空，A股T0也可以多空。<b><font color=blue>总之我认为截面构造多空组合属于套利范畴，之前是pair，然后策略是一个做多一个做空，现在就是放大到两个组合，一个做多一个做空。而且抛开什么统计模型计算fair price，套利最本质的内涵就是空手套白狼，就是无论市场是下跌还是上涨，我都能赚钱，而且都是赚取的无风险/低风险收益，就是从一个0的初始值能产生一个正利润，FIN2020也讲过，只要当前的初始资产为0，然而未来存在一种possible state下的收益为正，那就是套利。这就是套利的本质，就是neutral，就是中性，就是对冲，多空组合这种属于money neutral，对冲策略则是risk neutral，套利就是中性就是对冲。</font></b>




#### 1.3 Paper: Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&P 500

文章写的很好，策略讲的极其清晰明确，所有模型，数据的调参细节都非常清楚。2016年的文章，引用量几乎是套利里面最高的，有六七百的样子。主要因为简单易懂。

**数据和特征工程**
- S&P500日线数据，750交易日数据训练，250交易日数据测试。没用滑动，只使用了不同周期的lag return作为feature，一共31个feature，lag period从1到20，然后40开始间隔20到240，最大跨度对应一个一年的lag return。数据选取时间段是1990到2015，由于在滚动的用三年的训练，一年的交易，相当于从1993年到2015年，每年都在用前三年的数据训练好的模型，去根据现在的日线数据去预测然后日度调仓交易。即使是每年都在用相同的训练模型，**最后结果是一个ensemble的模型的废后年化有七十多，不过这个也不足为奇，因为文章结尾也说了，主要的收益都是在2008年以前赚的，在2008年到达的顶峰**。这个正好和文艺复兴基金那段时间赚的七八十的年化是匹配类似的！也就是机器学习在08年前特别赚钱，后来收益就越来越低了，因为都在用ML在做预测了，不然也不会至于这种一整年都在用同一个模型还能赚这么多钱的……
- 这里的feature可以理解为简单的动量特征，因为lag return本身就是一个动量的反应，不同period对应不同时期的动量。如果是日度数据，那么这些动量特征其实是很经典的，就是先取daily lagged return从一天取到20天，然后是monthly return，从1月一直到12月。
- 注意预测的target，target不是预测涨跌或者预测收益率，而是预测是否超过了S&P的平均收益，二分类问题。不使用回归问题，因为文献普遍表明回归的效果远不如分类的效果。而且这样的预测标签并不影响策略的制定和收益，因为策略里股票的选取是根据predict的概率来的，而不是这个0，1的标签，此外策略是多空的，也就是不会说熊市都在跌那就没钱赚了，这种多空本质上就是套利，毕竟套利最本质就是空手套白狼，这种dollar neutral，初始资产为0最后多空换来了正收益本来就是套利，或者就叫美元中性策略。




**优化器时间线**
- 1951年：SGD。`A stochastic approximation method.`
- 1986年：Momentum。`Learning representations by back-propagating errors`
- 2011年：AdaGrad。`Adaptive subgradient methods for online learning and stochastic optimization`
- 2012年：AdaDelta。`ADADELTA: an adaptive learning rate method`
- 2014年：Adam。`Adam: A method for stochastic optimization`
- 2016年：优化方法综述。`An overview of gradient descent optimization algorithms`
adagrad的最大缺点就是不能从local minimum中挣脱出来，而SGD根据其随机性，是可以从local min中挣脱出来。

**DNN**
- 使用了dropout和adadelta方法。AdaDelta优化器是在adgrad上的改进，目的是改进adagrad过激的单调下降的learning rate。 Adadelta不积累所有过去的平方梯度，而是将积累的过去梯度的窗口限制在一定的大小。同时adagrad对超参数的选取非常敏感，尤其对global learning rate非常敏感，而adadelta做了robust处理，对超参数并不敏感。dropout是为了防止过拟合。
<div align=center>
<img src="./figures/adadelta.png" width=70% />
</div>
- 结构是31-31-10-5-2，input dimension对应feature数量，第二个31说是根据heuristic，使用和input一样的dimention可以避免过拟合，第三第四就强制降维。hidden层的dropout取0.5，input层的dropout取0.1，就是有一定概率扔掉一些feature。DNN训练加了L1正则化，rate为0.00001，也就是对cost function加了个L1正则化，目的也是防止overfitting。L1就是lasso，参数会直接逼近0。训练400个epoch，同时使用earlystopping机制，只要最近5轮的loss的平均值，连续5个scoring event下没有提高，就停止训练。作用也是防止过拟合。

**Boosting**
- 用的是adaboost，每次用一个weak classifier，就是一个小的生成树。在stochastic gradient boosting的基础上做变化，使用类似random forest的方法，每次split的时候选取一部分子集的feature。一共四个参数：1.boosting iterations，2.tree depth，3.learning rate，4.number of subset features used at each split。不知道怎么实现这个奇怪的随机森林+boosting的东西。iterations取100次，防止overfit；每一轮的tree depth是3，不取1是为了有一个two-way interaction；学习率公式似乎和神经网络类似，和iterations是反比，常数项是10左右，所以取10 / 100 = 0.1；每次用15个feature，也就是一半去训练。
- 是我肤浅了，看sklearn的document里面，`sklearn.ensemble.AdaBoostClassifier()`和`sklearn.ensemble.GradientBoostingRegressor`以及`sklearn.tree.DecisionTreeClassifier`，里面的参数说的很清楚，adaboost本身的参数有iteration的数目，就是`n_estimators`，有adaboost的算法选取，有{‘SAMME’, ‘SAMME.R’}两种，有`random_state`和`learning_rate`，同时需要传入一个`base_estimator`，这个参数就可以自己指明每轮训练使用的weak classifier，如果是None则默认是一个树桩，否则可以传入比如DecisionTreeClassifier，那么对应的参数就有`max_depth`就是上面说的，各种其他的feature比如说`min_sample_leaf`，还有`max_features`，这个max_features对应的就是The number of features to consider when looking for the best split，也就是上面的那个每次选取的子集feature的数目！！也就是sklearn本身就可以实现这些东西。而如果是GBDT那参数就更多了，有`learning_rate`，有`n_estimators`，有一个参数叫做`subsample`，这个是adaboost没有的，而这个参数如果小于1那么就相当于实现了stochastic gradient boosting，也就是下一次只选用一部分的subsample去拟合，而adaboost没有这个模型因为adaboost定义是需要每次都用到全部的样本。同样也有`max_depth`默认为3，实现每个独立的weak classifer的定义，同样也有`max_features`就是对应每次split取多少个feature，可以自动选择"auto"，可以自己指明整数等等。也就是以上的各种功能全部都能很好的实现，还有很多其他的参数可以调。
- 总而言之就是注意boosting本来就是一种sequential性质的模型，所以每次用到的base model一定要简单，不能复杂。

**Random Forest**
- 常规经验参数，训练1000个树，不会overfit，每次树的max_depth都是20，然后每棵树都选用根号p个feature，也就是根号31。

**Ensemble**
- 使用了三种集成方法，这个集成还和之前学的投票分类器不太一样。sklearn中的`sklearn.ensemble.VotingClassifier`，决定软硬的参数是`voting={'hard', 'soft'}`，硬投票分类器是对每一个prediction，都取majority vote，比如两个投1，一个投0，最后的分类结果就是1；软投票分类器就是根据概率，比较科学，比如两个投1，一个投0，但是投0的预测概率是0.999，那最后分类结果仍然是0。注意只有votingclassifier有这个性质，votingregressor没有，只会把各个分类器的结果取平均输出，最多有一个参数`weight`，决定不同的子分类器的权重。
- 这里的集成ensemble是作者自己写的，把RF，Adaboost，DNN的预测的probability取出来，自己加的权重得到新的probability，然后看是否大于0.5，大了标签就是1，小了就是0。这个程序也很好实现，知道三个分类器的预测概率即可。然后不同的权重可以得到不同的ensemble模型，而每一个不同的ensemble模型就是一个单独的模型。这种概率加权的方式和投票分类器显然不一样，而且听起来还挺科学的，未来我也可以试试。提了三种ensemble的权重的方法，1.等权重；2.根据模型在整个训练集上的基尼系数加权，基尼系数越大说明模型区分度能力越强（见文章批注）3.根据基尼系数的ranking，取到数做加权。
- <font color=blue>使用ensemble的意义:一个是均摊风险，防止某个模型不适用而影响整体，另一个更科学的原因是说，这些单个模型容易困在Local minima，比如神经网络，优化器选不好那就会困在local min，比如adagrad，甚至是adadelta也很容易困在局部最小值，甚至是SGD，也有这个风险，更不用说树模型，无论是adaboost还是随机森林，都使用的是贪心算法，那贪心就是最容易困在一个local min。所以很多机器学习竞赛最后各种模型都试过之后都会加一个手动的集成，比如之前optiver那个lgbm-baseline，最后就输出的是一个KNN和一个神经网络预测值的平均，那个就是一个ensemble，当然和votingregressor也挺像的。</font>

**交易策略**
策略就是一共3+3=6个模型，对测试集每一天，计算之前的需要的lag return，然后扔进已经fit好的模型里面给出预测，六个分类器，每个在每天都能给出一个预测的概率，我们根据这个预测的概率排序，然后做多前K个股票，做空后K个股票。K是一个参数，当然自我感觉也可以考虑根据预测的概率然后加一个阈值来做多做空，或者划分组合，做空top的做多bottom的，这个就比较灵活了。然后比较这六个模型哪个好，顺便比较下K值对结果的影响。每日预测，每日调仓。



**模型结果**
- 六种模型无论在任何K值（取了10，50，100，150，200），directional accuracy都大于50%，注意文章预测的target variable是是否股票表现超过平均水平，而不是是否涨跌，所以accuracy是和实际是否超过平均水平计算的一个准确率。说明平均而言模型的预测准确率高于random guess，一般都是五十二五十三左右。K越大，average daily return越低，毕竟我们引入了更大对于涨跌的uncertainty的股票。
- ensemble模型普遍比单个模型表现要好。集成模型表现优于单个模型的原因是：1.子分类器足够的diversify。也就是不同模型的error的correlation应该低相关。DNN，RF，Adaboost本就是完全不同的模型，这点可以得以保证。2.子分类器全都足够准确。这里三个base learner都达到了不错的超过50%的预测accuracy，那合在一起也不会差。三个不同权重的ensemble表现类似，其中simple average就表现很好。
- 树模型相对比较好训练，随机森林的表现优于梯度提升，又优于DNN。猜测DNN的参数太难调，feature中存在太多的noise，但至少文章提出的这个baseline model表现并不好。
- 模型之间的对比就用一个panel计算了K的影响，不同K的平均daily return，日收益的标准差，accuracy，然后固定K=10下详细统计**几种模型在费前和费后的多空收益、总收益、总收益std、t值、收益分位值、偏度、峰度、PT检验、NW检验，不同分位数的VaR和CVaR，最大回撤、calmar ratio，夏普比率，sortino比率。**
- 几种模型表现的特点就是，VaR比较大，比pair trading大很多，当然收益也大。另外就是最大回撤特别大，基本都在70%以上，甚至DNN的最大回撤达到了95%（就TM离谱这个回撤，基本上就是全部亏完了，跟ETH一样呢。不过基本上见过的ML的策略都是回撤大的离谱……），然后算下来calmar，也就是收益回撤比有个0.99，就是收益大回撤大，显然不是一个好的策略。然后夏普有个1.81，全靠return撑起来的。**这个收益回撤比的意思就是，发生一次这最大回撤，我得用整整1年的时间才能赚回来，才能recover，因为calmar的分子是年化收益**。这个回撤太劝退了，虽然年化收益ensemble也有个七八十。
- **文章进一步计算策略的alpha值，也就是对systematic risk的宏观因子做一个中性化，检验残差的显著性**。把等权ensemble的after transaction cost的收益和传统的fama三因子、五因子，还有自己加上的短期momentum和reverse因子做多因子回归，还有加上了VIX相关的一个dummy variable回归，看模型收益在这些宏观因子上的暴露情况。文章里面贴了fama的三因子五因子下载的地方，fama的website。**回归后的残差就是策略的收益中不能被系统性风险解释的部分，就是我们说的alpha**。回归结果发现，常数项alpha都是显著的，同时其他一些宏观因子的loading也是显著的，对与市场回报率因子的loading是显著的，因为我们的策略是dollar neutral的，不是market neutral（也就是portfolio的初始资产为0，但并不是组合相对market的收益为0）。**同时自己添加的短期动量和反转因子是显著的，这个很好理解，因为本身机器学习就是捕捉趋势的，但是复杂的ML模型同样捕捉到了动量和反转以外趋势信息，所以去除了动量和反转因子后仍然有显著的alpha，不过alpha比用其他因子模型就要小了，因为一部分被动量和反转解释了**。VIX也有一定的解释力，**VIX本就是专门针对标普500制定的隐含波动率指数，volatility inflation，体现对未来30天的市场波动率的一个预期，也叫做恐慌指数（investor fear gauge）**。这个波动率指数系数也是显著的正数，说明ML模型成功捕捉了一些波动率上的信息，波动率越大，ML模型表现越好。

**分时段分析**
- 笑死了，这个彻底蚌埠住了。我就说这些简单的模型，一个二分类怎么会有效。原来1993-2001年赚取了几万倍的收益，然后后来越来越不行，2010-2016年直接净值从1跌到了0.3，而这段时间市场净值都翻了一倍，从1涨到了2，真TM丢脸，真就亏成狗。简言之就是本文的策略没有任何的卵用，上面所有好看的数据全都是靠二十年前回测堆出来的，一大堆什么fancy的ensemble，好看的收益全部都是二十年前的，垃圾东西，我就想这个东西怎么会有用呢？况且九几年赚的那么疯狂，他最大回撤还能到百分之九十多，收益回撤比小于1，所以说是真TM没用啊。笑死了，只能说从文章里学到了很多关于机器学习模型的话术，分析方法，怎么把一个套利策略给设计的能写这么长一篇文章，顺便帮忙温习了一下简单的ML，为下一步搞模型打基础了吧。笑死，除此以外没有什么作用了。
  
**总结**
- 还是学到了很多话术和机器学习简单模型的复习的。文献里出现这么简单的ML模型的真的不多了。重点温习了机器学习，别的策略上给了点insight吧，怎么ensemble模型一类的。
- 最后结果奇差无比，回撤也大的吓人，不过可以理解，因为一方面ML模型基本上回撤都不太行，另一方面本文的模型实在太简单了。三点，1.拿三年的回测，然后一整年都不换模型，这效果能好？注定会死的很惨啊。2.feature都太过简单，一共31个feature，竟然全部用的lag return，任何其他的价量信息都没用，甚至没有用任何的统计函数去构造一些因子，只用了一个收盘价序列，效果怎么好？而且因子也完全没有处理，就直接拿来用了，没有任何的滑动平均优化，没有因子挖掘等等； 3.模型也都是最简单的模型，基本这种单纯的神经网络表现奇差也能理解，因为以前的文献就没见过DNN和LSTM在预测股市上有什么作用的，果然表明结果差的一批，收益也低，回撤还九十多，笑死了。哪怕是加了什么earlystopping什么dropout，什么ADADELTA算法，没个卵用。其他的树模型也就仅仅是简简单单的设置了一些参数，大部分都是default，也没有做任何样本内外的参数优化，什么网格优化，K折交叉检验超参数调优，贝叶斯优化，随机优化什么都没做，全部都用的default，模型这么简单也不会好。至于这个训练label是不是不行我不清楚，感觉还好，这种简单的多空策略，毕竟用到了预测的概率去制定的。


**补充一些知识**
- VaR在险价值，CVaR条件在险价值。
- 基本上各类机器学习模型都可以加一个regularization。比如神经网络，文章里的DNN就加上了一个L2的正则化，这个在torch或者tf里面应该很好实现。同时logistic也可以正则化，在sklearn的`sklearn.linear_model.LogisticRegression`中，参数`penalty`的默认值就是l2，也就是l2正则化。所以之前做作业看到sklearn的结果和statsmodels的logistics不一致，原因就在于sklearn自动加上了L2的正则化。


#### 1.4 Paper: Statistical Arbitrage in Cryptocurrency Markets
2019年的文章，基本是在仿照上一篇。引用量有个三四十。除了使用的是分钟数据，然后把数据周期换了一下，特征基本一致，每次预测是未来两个小时40个币种中超过average表现的情况，然后取前三名后三名做一个多空组合，用的模型是logistics和随机森林。logistics用的L2正则，优化器使用的是L-BFGS。

文章有代码，代码在[Statistical-arbitrage-in-cryptocurrency-markets](https://github.com/Exceluser/Statistical-arbitrage-in-cryptocurrency-markets)建议好好读读，代码写的很好，除了没有数据，基本完整的逻辑都实现了，预测，包括数据处理和模型都有，我提了一个issue给作者要数据，如果不给也没办法，可以试着用ccxt接到数据复现一下，但整体代码写的很好。


#### 1.5 Paper: Deep Learning Statistical Arbitrage

**文章介绍**
- 2021年最新的文章，没什么引用量，但是文章的模型看起来很fancy。之前量化投资机器学习的公众号发过有NVIDIA的人解读这篇文章，说是提供统计套利的最新方法和动向，当时没参加。没什么引用量但是github上的star数量倒还有好几十，看来还是挺多人感兴趣的。
- 文章有代码，作者公布代码在[https://github.com/gregzanotti/dlsa-public](https://github.com/gregzanotti/dlsa-public)，可以通过github desktop直接拷贝到本地了，不过模型看起来比较复杂，并且repo里面没有给数据，issue里面去年都有人提了，估计是没戏了。而且里面大量的TODO的注释，说明代码并没有完全优化。不过本身代码就用了很多fancy的东西，我还不是很了解，比如yaml的模型config文件，还有各种数据库储存等等，有空还是学习一下吧，感觉写的class非常正统，非常符合这种paper的源代码格式，各种文件，class的封装，写的非常规范，包括写论文包括实习的文件都应该这么布置。   
- youtube有作者presentation slides和presentation现场，结合pre去学paper效果最好，而且如果未来要做比如整理我这段实习所了解到的全部有关套利的东西，那么这个pre slides还能用的上哈哈。 


**前言**
- 统计套利的simplest form是pair trading，找到similar asset，利用cointegration关系，对价差建模，超过一定threshold就做空winner做多loser，这样在价差产生mean reversion的时候就可以平仓获利。**本文主要回答两个问题：一个成功的套利策略需要什么成分；以及当下的市场还有多少realistic的套利机会**。
- cointegration，协整关系。协整可以理解为平稳性的进阶版，在课内我们只讲到了平稳性没有讲协整关系。我们在之前做CTA时序因子的时候都在检验因子的平稳性，包括会检验期货时序对数收益率的平稳性，**因为只有收益序列是平稳的，才有预测的可能，才有回归分析的意义，否则所有使用收益作为Y值的回归都是伪回归，因为过去发生的事情未来根本不可能发生**；而因子是平稳的就有好的性质，过去发生的事情才可能继续发生。平稳序列就具有mean reversion的性质，而pair trading常说的两样资产的价差平稳属于一阶的简单系数的协整，协整更广义的是只要两个序列的线性组合是平稳的就可以。检验协整关系使用coint，检验平稳性用单位根检验，最典型的单位根减方法就是ADF检验。使用statsmodels检验协整关系：
```
# coint函数检验协整关系，p值越小代表协整关系越强，H0是没有协整关系
from statsmodels.tsa.stattools import coint
p_value = coint(X, Y)[1]
```
**统计套利的三要素**
1. Arbitrage portfolios。套利最本质的就是空手套白狼，从0到正收益。那基本会涉及多空（leadlag除外）。配对交易的多空都是单个资产，但更广义的多空资产应该是两个组合。如何选择多空组合，也就是如何定义similar？（简单的配对交易就只看时序的相关性，比如日线同频的话只算一个correlation，高频不同频的话用previous tick或者HY去衡量）
2. Arbitrage signal。我们要对价差，或者一个线性配比下的价差做时序建模。那么用什么复杂的模型建模？以及建模后我们要确定交易信号，什么时候开仓什么时候平仓？（简单的配对交易就是价差是一个white noise sequence，那么服从近似正态分布，在超过一定的置信区间范围就会开仓和平仓）
3. Arbtrage allocation。把market frictions, commission, microstructure noise, market impact等各种东西当作constraints，那么我们要做的应该是一个优化问题，就是在给定的这些constraint下，如何进行仓位管理，如何设计交易模式和交易量去maximize a target function（trading objective）？

**当前的困难**
1. **Large number** of assets with **unknown similarities**
2. **Complex time-series patterns** in price deviations
3. Optimal trading rules are complicated and **depend on trading objective**

**一些想法**
- 本身similarity就很难定义，单纯就return的时间序列上，可以定义说线性相关，或者什么时序上的相关性比如傅里叶、互相关，HY相关，但我们一般pair trading也会找同一个板块的股票，那就是股票本身的基本面，流动性等等一些characteristic。这就比较难以定义，但也是在衡量similarity的时候需要考虑的因素。
- 我们之前美赛就是做的对交易策略建模。往往这个过程中就发现很难有数学的部分，包括读了这些套利的文章，基本上数学的部分集中在比如HY函数的底层逻辑（结合到价格模拟的GBM的一些伊藤积分，还有一些线性回归模型的矩阵运算和推导），其他文章基本上数学的部分只会在机器学习模型的时候结合着模型写一些公式，大多数情况数学都很少，这也是我们当时建模遇到的困难，因为平常的因子选股，策略开发，简单的套利策略都不是很好量化成数学公式，都是一些用来理解的交易逻辑，包括CTA，基本上时序因子挖掘，最多就因子计算上面设计到一些数学公式，EWMA等等，还有因子挖掘会有一些公式，模型上有一些公式，真正的逻辑，交易逻辑，回测这些都没什么公式可以写。大量的数学公式集中在衍生品的pricing model里面，这些期权期货的定价模型需要数学理论支撑，但是交易策略本身没有。除了这些基本上涉及理论最多的部分就是组合优化，组合优化里面涉及到的马科维茨均值方差、风险平价、对角线风险平价、夏普比调仓、主成分分析、什么凯利公式这种。此外就是一些评价指标计算，比如VaR，sortino比率，calmar等。当然机器学习模型本身那数学挺多的，讲优化器原理什么的。当时美赛的数学部分全靠1.因子施密特正交化和因子挖掘；2.马科维茨优化。用到的也基本就是投资组合的理论，我们FIN2020和3080讲了马科维茨，以及对应的一个QP问题，他们4800讲了一个针对risk preference的优化问题，把整个做成一个优化问题，当然这个优化问题本身没有考虑手续费，虽然是一个constrained optimization，而只是按策略交易之后再去扣除手续费看结果。
- 但本文的数学公式很多，很适合拿来做建模（MLDL的模型在实盘就可能比较困难，尤其是要处理组合优化后的小数点权重的问题等等），涉及到投资组合理论（多因子模型），涉及到简单的深度学习工具，以及投资组合权重优化。
- 在之前的两篇文章当中，使用的ML模型只是在预测是否outperform the market average，然后去构造多空。但是如果是对价差建模，那就不是一个简单的prediction或者classification问题。本文中**We use a trading objective on residuals of asset pricing models**。也就是相当于是最终还是在解决一个优化问题，和之前的美赛东明做的组合优化问题类似，比如objective是一个risk-adjusted return，像夏普，或者其他的函数，当时美赛的objective选取的是一个mean-variance objective，本质就是自己搭建的一个utility function。然后再mean-variance下这个utility函数取的就是课内的最经典的（FIN2020和STAT4012都讲了的），去在给定的return下最小化Variance。权重大于零就是不能short sell。之前也是因为这样一个明确的优化问题，东明才会去想着用强化学习的模型去拟合一下。
$$
\min_w w^T\Sigma w, \text{ such that } \mu^T w = z, 1^Tw = 1,w \geq 0
$$

写一下KKT条件，等价于（算是duality吗？）
$$
\max_w \mu^Tw-\frac{1}{2}\gamma w^T\Sigma w, \text{ such that } 1^T = 1, w \geq 0
$$
本文用的模型与这基本大同小异。

**方法综述：Deep learning statistical arbitrage**
1. Statistical factor model including characteristics to get arbitrage portfolios
2. CNN + Transformer to extract arbitrage signal: Flexible data driven time-series filter to learn complex time-series patterns
3. Neural network to map signals into trading rules: generalization of conventional "optimal stopping rules" for investment
 Optimize and integrate for global economic objective: **Maximize risk-adjusted return under constraints**
 Most advanced AI for NLP for time-series pattern detection
 
- 套利的对象选取的不是简单的一个预测的多空，而是通过和风险因子模型（加上自己定义的conditional/unconditional的统计因子比如short-term reversal/momentum）计算取的残差组合。
- 时序模型建模并未使用经典时序模型比如GARCH或者ARIMA等等，因为这些模型往往都需要提前在一段时序上fit好，也就是提前把parametric models的参数固定，再去fit，而且就算我们用什么滑动窗口动态调arima，基本上时序的参数也不变。也没有用一些比如GAM, general additive model或者其他的一些regressor去fit，因为作者觉得这仍然相当于是在静态的去给一些basis function去拟合上一个系数。就算你rolling的调整，也不好。因此真正想实现一个动态建模，就需要类似强化学习的一个data-driven way的方式去解决。文章使用的convolutional transformer也是一种data driven method。



**数据**
- 使用的是十九年的500个流动性最强的股票的日线数据，考虑了各种风险因子模型（比如Barra），对比了不同的金融数学里面的mean-reversion models (parametric/non-parametric)。结果是substantially outperforms all benchmark out-of-sample，年化夏普达到4，年化收益20%，波动率少于6%，同时收益和常规风险因子以及market movement无关（也就是不被宏观因子解释，属于是alpha部分的收益），同时扣除了交易手续费。同时stable over time and robust to tuning parameters，这个是我最关心的，对调参比较robust是大多数ML模型都能实现的，但是stable over time就很困难了。

**关于residual portfolio与factor loading**
- 文章强调了为什么用residual portfolio。我记得pre里面主持人就提问，她怀疑这个策略能否具体的implement，因为这个residual组合并不是很好交易，似乎作者给出的回答好像是使用ETF什么去近似，不是很理解。文章说用residual因为多数情况下，residual是stationary并且低相关，无论是时序上还是截面上。对这样的residual的return做时序建模就有意义，因为历史发生的事情未来也会发生，**而如果直接对日线return去用ML模型建模预测，基本不可能有好效果。这个也很好理解，直接对日线数据做什么时序建模，无论用什么时序模型GARCH什么ARIMA，他大概率不平稳，然后你又要用trend先去decompose，这一通操作乱七八糟的基本上结果就不会好，之前的两篇文章直接对日线的return拟合机器学习模型，基本表现稀烂，因为日线return就是由几个systematic factor在解释，时序上也可能具有一定的自相关性，而且由于行业板块的存在，轮动的存在，注定会有很大的heterogeneity，整体来说就是噪音太多不便于建模**。<font color=blue>所以这里用residual建模倒可以理解，不过我想如果是高频的日内数据，比如高频的期货数据，那么之前检验过基本上由于时间频率升高，受到的noise会小很多，因此之前CTA里面也测试过多数的时序是平稳的，然后间隔取降低频次，发现就是频次越高越不容易平稳，最后到4096个tick基本上对应半小时，就已经数据有些不平稳了，更不用说日线数据了。我们之前3080检验过daily return的一个分布情况，做过normal的分布检验，结论就是有很大的偏度和峰度，而且有很多outlier，拒绝假设检验，正态拒绝了，基本就不太可能是弱平稳了。那根据这个我想，这应该算是高频T0的一大优势所在，大量失效无用的日线时序模型、机器学习模型，也许在T0中就会有用。毕竟文章这个跟风险因子模型回归取残差，风险因子的计算值比如barra，比如fama-french，最高频也只有日度的吧，所以日内肯定是没法用的，但既然高频的log return序列本身就比较平稳的话，那是否可以跳过第一步直接尝试后面的步骤？</font>
- **这里和课内知识串起来了**！！！！<font color=blue>如果使用了风格因子做回归，那么就有了similarity的衡量，也就是对于risk factor的loading是相似的，这就说明两样资产的基本面信息，systematic的部分，beta的部分是相似的。那么这样构建多空组合，超额收益就是纯alpha，就是纯套利。这个完全符合课内讲的，2020课内讲反证法证明APT的时候，也是控制两个portfolio的对单因子/三因子的loading一样，然后做差证明存在套利机会，这本来才是课内讲的最正统的套利啊！包括3080我们APT那一章也考过通过系数配比，构建factor loading，验证是否有arbitrage的机会，一模一样！还有我们IVOL因子的检验，构建多空组合，也是通过调整对IVOL最小的组合和最大的组合的权重，使得这个配比权重下实现一个前面所说的market neutral，也就是整个多空组合针对market单因子或者fama三风险因子是中性的，然后做多空组合，那么收益就算纯alpha，算的就是纯因子收益的显著性。这下就把课内的知识，project的知识，和这篇论文串起来的，而且也把单因子检验、多空组合、中性策略和套利给串起来了。那这么看来，IVOL这个单因子的检验过程就是一个最正统的统计套利，我们在套一个real price和fair price的利，而这里fair price就是由多风险因子模型决定的。同时这么看来，前一篇文章的那个多空组合也算是套利，不过不是market/factor neutral，而是dollar neutral，文章也是这么说的，之前一直纠结这个算不算套利，因为不知道这种case下所谓的fair price是什么，现在看来就懂了，factor/neutral下的fair price是多因子模型算出来的，而dollar neutral下的fair price就是0，也就是多空组合的价值之差的fair值是0，如果不是0，我们会期待他回到0。所以在factor neutral下，我们对于多空组合的仓位分配是不同的，目的是为了凑factor loading一样，但dollar neutral下，我们对多空组合的仓位分配是相同的。</font>
- 那这么说，这种最正统的factor neutral的套利只能对日线数据使用，而且只能是股票市场哈哈。而dollar neutral虽然不算，但日内高频的套利能用啊。所以文章使用因子模型的loading作为similarity的衡量是正确的，目的就是通过系数配比，把long-short portfolio对因子的loading抵消掉。

现在的疑问就是，怎么选套利的组合？loading不可能完全一样，难道就几个loading去算correlation？以及谁多谁空？是回归常数项为正的就多，为负的就空吗？


**模型**
还没看完


#### 1.10 关于套利的思考与总结

如果让我把实习和之前了解到的所有套利相关的理论、概念，以及阅读过的统计套利的文献，做的leadlag研究等全部总结成一个大的PPT去present，会怎么去讲？

- 关于套利的基础理论：FIN2020的内容，设计套利的部分，arrow-debru理论，martingale理论，**APT模型与多因子模型**

- pair trading
- 套利的必须要素
- 可能有什么套利策略？多空组合，结合论文、论文presentation的PPT、点宽对于CTA套利的讲解，babyquant的CTA的套利部分。
- 币圈的套利（期现套利、跨期套利、跨交易所套利），最好部署实盘的套利策略？
- 复杂的机器学习和深度学习怎么帮助套利？



### 2. 经典套利策略回测


#### 2.1 复现crypto arbitrage文章的策略
目前看的文献当中, 最容易implement的就是statistical arbitrage in cryptocurrency market的策略, 也是美股那一篇已经证明过无效的策略。但是当时毕竟使用的是日线数据, 而且作为最经典的套利策略, 有必要去复现一下框架, 而且里面用到的ML模型也可以和CTA部分的ML/DL模型呼应, 形成相互促进的关系。而且本文github有复现的代码但没有数据, 因此首先要复现的最适合高频的最简单的套利策略就是这个。 





---

<div align=center>

### CTA策略研究
</div>

#### 1. 基本思路






---

<div align=center>

### 实习感悟
</div>

**关于传承**

之前实习生开会的时候，万总也一直说到这个问题，很多量化公司不想招实习生，要么就招技术能力很强的来帮忙，要么就自己带策略来。因为公司发现一开始培养人才，手把手教，培养一个量化研究人才往往需要几年的时间，然而这个过程中，是没有绑定的，也就是说没有机制去绑定这些人，因为他们在这个成长的过程中，在变强，但是没有到能发产品的水平，也就是导致成长之后，就会走人。而如果他们有着成熟的策略，公司会给他们一部分的股权奖励，也就是我们说的合伙人的一个机制，这样就能把公司成长和个人的成长绑定起来互相激励。也就是盛冠达之前的这个传承精神做的并不好，员工都没有传承意识，公司培养成人才之后，每天各种猎头来骚扰来诱惑，拿出更高的薪水就跑了。这就缺乏一种发自内心的对公司的感激和对企业文化的认同，从而导致了传承精神的缺失。


现在白鹭到了百亿，看实习生的简章都是要求6个月以上，聚宽也是，诚奇也是，因为这样才能比较完整的培养一个量化人才，三个月的时间实在太短暂了。往往三个月的时间，实习生干不出什么东西，而且在三个月的时间内，很难让一个实习生对整个公司的文化有一个非常深入的体会，那么如果在人才培养的过程当中，没有形成这样的企业文化的认同，那最终人才留存率是很低的，而恰恰量化这个行业是吃经验的，这样就很吃亏。我也希望能去到白鹭实习，我也希望能在那里实习六个月，因为我很喜欢白鹭的企业文化。其实在盛冠达，我签的合同是六个月，现在我基本快干满三个月了，我说实话感觉才刚刚开始入门，刚开始了解公司的回测系统，企业文化，人才的培养逻辑，员工和实习生的身份认同，还有刚刚开始入门深度学习时序预测，开始编写策略。所以我认为大学生的量化实习，那种两三个月的都是扯淡，量化就是搞科研，你搞科研搞了两个月能搞出什么东西？所以很多量化私募要求半年实习，我也非常认同这一点，即使我签的合同是三个月，公司不赶我走的话，我一样会做下去。

当听到最早使用sunnytrader的一批人都走了之后，我才算体会了黄总的心酸。自己写的交易系统，开发成长，结果元老级别的一批人物都走了。这种离职的事情我以后肯定也会遇见，这也是每个创业团队都需要经历的成长吧。

这种内部开发的研究系统，需要开发一个完整的文档出来。公司能把这拿出来给实习生用，已经很感激了，我之前是知道博普的实习生是没资格参加周会的。我们在这里能用到正式员工用的回测框架，用到公司的服务器，已经是万分感激了。盛冠达如今的公司文化我挺认可也感觉十分的舒适，相比于中信证券，我没有看到中信证券有任何的公司文化，垃圾玩意。