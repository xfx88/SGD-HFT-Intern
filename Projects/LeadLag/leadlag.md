<font face=''>

<div align=center>

## Leadlag Research
</div>

---

<div align=center>

### 全A股领先滞后研究

</div>


### 1. 核心文献
#### **Paper: High Frequency Lead/lag Relationships Empirical facts**
#### **Paper: Ultra High Frequency Statistical Arbitrage Across International Index Futures**

基本是HFT里面讲leadlag的引用量最高的文章了，第二篇是完全基于第一篇开发的超高频的一个跨市场回测。分别是2012年、2013年的文章，引用量有70多。有点老旧了，但是方法非常经典，现在研究的也不是很多，大多数都是在一些国内的博士论文里面瞎水文章水出来的，往往都是在研究期现货的leadlag，然而交易操作难度很大，纸上谈兵没个卵用。文章有点早，也所幸有人开发了HFHD这个包，快速的计算HY互相关，给我的工作带来很大的帮助，不然我可能三个月就去自己手写然后优化一个小工具去了，根本做不到策略。当然他的包也不是完善完整的，直接运行也会因为数据格式的问题报错，我在里面debug了，加了自己的函数和代码，方便一扫多的需求使用。

**文章结论**
- 期货和对应的股票有高度不对称的互相关信号。
- 流动性更强的资产（相邻两笔成交的duration时差短，bid-ask spread小，波动率低，换手率高）更倾向于领先流动性低的资产；然而相同流动性的asset的互相关性更强。
- Lead-lag关系难以捕捉，日内的lead-lag并不持续，具有日内周期性，比如宏观数据的公布或者US开盘的时候会比较明显。
- 数据越高频lead-lag关系越明显，使用leader的历史数据去估计lagger的next midquote variation，做一个Midquote的涨跌预测（二分类问题）可以达到60%的accuracy，远高于lagger自身的历史数据。而当从tick data变为minute data，频率降低时，lead-lag relation消失。
- 然而单凭这样的预测去下市价单，会吃不过bid-ask spread赚不了钱。

金融市场最著名的APT，no arbitrage若成立（即可以搭建zero-cost portfolio实现在未来有非零的概率获得正收益），那此时股票和指数的价格序列画cross-correlation function（两个时序的不同lag trading days的）的结果应该是一个Dirac delta脉冲函数（顶点为1，其余为0）乘上两样资产的daily return的相关系数。除了lag=0处以外其余部分应该是类似white noise，绝对值范围在2/sqrt(n)以内。
日度的数据基本没有lead-lag relation，但高频的数据就有可能。

数据：
- Trade和quote（best bid and ask price and quantity），没有用到orderbook的多级行情。Quote往往多于trade，数据合并时以trade为基准，去匹配向前的最近的quote的数据。sampling schedule是依据高频的trade（resample time）去匹配发生的quote的数据，也就是说有的quote数据会用不到。如果两笔quote都夹在两个trade中间，那只会取离后一个trade最近的那笔trade。相当于observation time就是我们的quote的time，然后sample time就是我们的trade的time，然后quote根据trade去resample，得到的是一个子集，可以叫做pre-tick time或者叫做sampled quote time。后续在处理的时候没有直接使用原先quote的observation time，而是使用的tick time，也就是mid-quote有变化的时间，所以频次会比原先的quote time要低，然后使用这个quote time再去做resample，得到resampled quote time。这样得到的resampled quote time更一定满足相邻的mid-quote都是不一样的。
- 同时把相同时间的成交给aggregate，成交量相加，成交价换成VWAP，即把相同时间戳的成交量加和，成交价聚合成VWAP，保证乘积（成交额）不变
- 研究股票之间、股票与对应期货的近月合约。
- 每天舍弃开盘半小时和收盘前半小时的数据（不过注意这是法国交易所，早上九点到下午五点半）
- 只考虑常规交易，不考虑大宗交易（block trades）或OTC（场外交易）
- 对于不同交易所的资产，不考虑交易所的时间延迟

**Liquidity衡量参数**
- Trade里相邻两笔成交时差的平均值delta_t
- Average tick size/midquote，tick size是the smallest possible price fluctuation for any particular asset，也就是价格变动的最小单位，除以midquote的，再取平均值
- Bid-ask spread除以tick size的平均值，即(ask-bid)除以价格最小变动单位，再取平均值。这里的spread应该没有除以mid quote，只是ask-bid
- Ask-bid等于最小价格变动单位的次数，表示为%
- Trade through的次数（trade price低于当前best bid price的次数），表示为%
- 日度波动率的一个模拟值：相邻midquote的变化量绝对值除以tick size的平均值。这个比值越大，说明midquote变得越频繁，价格波动越大
- Turnover per trade，每笔交易换手率，price * trade_qty
所有需要算average的指标均以日度为单位独立计算，然后对所有天求平均，相当于每笔trade或者quote取平均，然后再对days取平均。

**Hayashi-Yoshida cross-correlation estimator**
1.	是一个unbiased and consistent的covariance estimator，主要用于处理asynchronous trading异步交易问题以及有microstructure noise的问题，也就是会有intraday variations in scale of noise，而且这种noise下的observation是离散的，不同步的时刻发生的。同时HY估计使用到了所有的tick data，可以理论上衡量最精确的lag time（异步交易难以衡量延后时间）
2.	paper《covariance structure》里面提出的这个HY关联函数，证明了在恒定波动率和correlation下这个estimator是consistent的，也就是数据量足够大的时候会逼近真实值
3.	HY关联函数的lag版本，就是这个paper用的互相关函数。maximize这个函数可以得到opt_l，这个opt_l最大化了相关性，记为两个时序的lead-lag time，本文第一看函数的不对称性进而判断谁是leader谁是lagger，第二看LLR函数的最大值以及所在点判断lead-lag关系的强度和最显著的lead-lag time。
4.	互相关的结果如果是高度不对称的，也就是说如果LLR远大于1，说明存在lead-lag关系，且X leads Y比Y leads X更加accurate。
5.	注意这个lag order L，这个L是只对observation time使用的平移。平移后重新计算overlap。

**模型**
- Ito process里的X，Y对应midquote，使用tick time而并非trade time，目的是做一个二分类问题。目的是估计均价在下一个tick time的变化情况，比如如果估计的下一个tick time这个涨的bps超过了比如一个平均的spread，那就买入，然后等到下一个tick time发生的时候就卖出套利。当然，这里就有两个问题，且不说不能在midquote交易本就是一个理想的情况。第一，下一个tick time是未来信息，我们需要估计。论文里使用的是当前时间加上相邻tick time的duration的平均值作为估计的。第二，二分类但我们做估计的时候似乎并不能使用其他机器学习模型，而只能使用线性回归，因为我们HY估计量的推导基础就建立在线性预测的基础上，如果用其他的预测方式可能不是很合理，而且机器学习只有leader的收益率也不是很科学，如果说延申的话可以考虑把leader的收益率和lagger自身的其他订单簿信息结合起来送到模型里面去预测。

**Intraday profile**
- 研究lead-lag intraday profile，发现lead-lag程度和news以及开盘高度相关。Profile本身和资产也相关。股票和股票，股票和期货算的intraday profile完全不同。Profile就是比如每五分钟取样，然后截面取平均，再在时序上相同时间段取平均。
- 计算5-min profile，日内做截面平均（所有的pair的LLR，然后对时序平均。
- 用来计算Intraday profile的lag grid只使用少于一分钟的，因为5min的profile比如9:30到9:35，我们就只能用这5min中的切片数据去计算correlation，如果lag太多，那么nan数据过多计算的corr就不准确。处理股票数据就发现，因为我们如果只取相邻midquote有变化的时间点，那5min的数据就更少了，计算出来的correlation没有意义。

**为什么不用pearson？**
1.	异常数据会干扰评估的结果，而高频数据里面存在microstructure noise
2.	假设数据都是同方差的，而这显然对intraday数据不适合
3.	相关性是全局同步性的快照测量法，所以不能提供两个时序信号的方向上的信息，也就是不能知道谁是leader谁是lagger。当然可以使用sliding window去计算局部的同步性，算一个滑动的平均，但无论如何都无法衡量有时间滞后的相关性
4.	只能衡量linear correlation
5.	使用TLCC（时间滞后互相关），可以计算两个时序的方向性，比如lead-lag关系，注意lead-lag关系并不能推出causal relationship，只能体现哪个时序更早的反应了信息。

**两个时序之间的检验**
1.	单位根检验，不平稳做差分
2.	协整检验cointegration，看两个时序是否有长期的关系，没有就用VECM看看短期的关系
3.	Granger因果检验，看X的过去值能否可以更好的预测Y未来的值
4.	做完granger可以做IR脉冲反应函数，这个图像一般最后是收敛的

**Granger causality test是什么**
在时间序列情形下，两个经济变量X、Y之间的格兰杰因果关系定义为：若在包含了变量X、Y的过去信息的条件下，对变量Y的预测效果要优于只单独由Y的过去信息对Y进行的预测效果，即变量X有助于解释变量Y的将来变化，则认为变量X是引致变量Y的格兰杰原因。虽然说是检验causality的但实际上只是在检验lead-lag关系，注意lead-lag关系并不能推出causal relationship，只能体现哪个时序更早的反应了信息。granger的基础是回归分析中的自回归模型，回归分析得到的是不同变量的同期的相关性，自回归模型得到的是同一变量不同期的相关性。
这个概念本身和lead-lag很接近，但一般只用在经济学变量的领域，同时需要时序是stationary的，没有考虑干扰因素的影响，也没有考虑时序之间非线性的相互关系。

**Epps effect是什么**
correlation estimates tend to decrease when sampling is done at high frequencies。
简言之就是宏观上高度相关的两个品种在微观上，比如tick级别是完全不一样的。很好理解，双胞胎长得一模一样，放到微观里面看细胞也是完全不一样的。也就是说，A,B两个高度相关的品种的价格时间序列，使用不同频率切片的数据去求相关系数，其结果是完全不一样的。切片频率越高，相关系数越低；切片频率越低，相关系数越高。babyquant测试期货的对数收益率，日内都是不平稳的，但平滑之后，或者每隔间距取一个点，再把时间拉长增多数据，放到更加宏观的层面，就变得平稳了。

**问题**
- 时序的这个互相关函数和CNN的互相关函数有什么关系？
- Paper里面最后发现有lead-lag的显著的只有股票和对应的期货，之前A股发现的lead-lag会不会只是因为“错误的计算”而发现的呢？因为对于之前的previous-tick estimator，得到的总是更加liquid的资产在lead不是那么liquid的资产。
- Previous ticks计算公式需要resample，而这个HY estimator公式本身看是不需要的，那到底需不需要？如果真的不需要为什么还要把quote给按照trade去sample了？
- 如果要找两个股票的lead-lag，那这两个股票的选取标准是什么？我们总不可能对所有的股票算pair吧？高频的本来就计算量爆炸。像pair trading好歹还可以验证比如correlation或者什么协整关系去选pair。哦懂了，根据LLR去画lead-lag network，画一个影响力网络，然后去看权重大的边。选取的对象可以是比如同一个板块同一个概念的股票吗？
- 具体怎么通过lead-lag关系去生成交易策略？这个lead-lag关系也会随时更迭，那我们是比如固定一个周期去重新画影响力网络然后选最新的pair？比如每天都重新评估一次然后选影响力最高的若干pair，下一个交易日去交易一天？至少文章里面的所有预测，然后下市价单都是亏钱的，当然可能是他选的那个pair不够好，但是总感觉有很大提升的空间，比如你不是每次预测涨就买，是不是应该超过一定阈值再买等等，以及他自己说的resample的时候用更宽的时间频率，比如不用一个tick（即midquote变化的时间间隔），而是用多个tick之后进行采样，效果可能会好？这样的话也有tradeoff，持有时间会变长一些，手续费吃得少，但是预测的准确率也会下降，因为时间拉长了。
- 这个能不能来搞股票和股指期货？
- 这个HY ESTIMATOR的计算能不能实现向量求解？本来数据量就很大，for循环太恐怖了吧。本身按照概念，是每一个相邻的都要去匹配和他有交集的，然后去相加。

**股票T0交易**
A股实行的是T+1交易制度，但这并不妨碍做T0。期货美股这些是做高频日内交易，指的是日初开仓，然后中间各种操作，不同的频率对应日内短线（持有几分钟），日内大波段（只抓当日的主趋势）还有日内中长线（持有一小时），然后日内交易需要当日清仓，相当于当日买当日卖。A股T+1不能当日买了之后当日卖出，而是针对那些我们打算长期持有的股票，而且是已经有底仓的股票。如果已经有底仓了，那我们就可以实现当日买当日卖，或者当日卖当日买。预期今天下跌，先卖后买维持仓位不变叫做“反T”，预期上涨后先买后卖维持仓位不变锁定利润就是“正T”。差价小，需要低手续费，印花税千分之一，一般要万二，千1.4的双边费用已经够高了。而lead-lag本就是T0最著名的策略，有点像事件策略，平常我们看机构买了我们跟，高管增持了我们跟，这就是事件策略，而T0短线里面，比如同一个板块，同一个概念和逻辑，利用板块联动的时间差做T，是最为核心的方法，其实就是用的这种lead-lag关系。
一天做T不宜次数太多，有确定性机会再做。尽量保持每日收盘的时候仓位不变，避免做隔日的T，更应该避免做隔周T，就是周五操作之后周一再完成一套闭环，周末信息的不确定性太大，不适合T0。同时T0一定要坚决的止盈止损，比如2%，也就是立刻恢复原来的仓位。



### 2. A股HY效果检验

我们使用中信证券600030和海通证券600837去扫全市场的A股，使用一天的快照数据，使用的是2022-06-01的数据。我初步截取了早上开盘9:30后到下午14:57的数据计算相关性。预测对象是midquote的变化情况，同样是使用tick time，也就是相邻Midquote发生变化的时点。我们会剔除掉tick time时间过少的股票（比如数量少于50条） 

由于是一天对四千多只股票，我们对股票做并行，使用24个核（后续如果是多股票多天数那需要并行的对象可能发生变化，整个全市场扫描计算HY estimator和LLR的过程可以被看做是一个三循环，先对天数循环，每天计算四千扫四千的股票的相关性也就是对股票pair做循环，最后是对grid做循环以使用HY相关性计算LLR。从体量来看，论文里面使用的是几年的数据但是他只考虑了几个pair，而我们交易要用的必定是大量的pair，而窗口根据算力不会取太多天，比如20天，甚至更短比如一星期，直观也容易理解，往往lead-lag pair未必是长期存在的现象，对任意的一个pair，你把时间拉长到一年，那计算的HY相关性的平均值都会很小，LLR的平均值都会接近一，因为我们本来用的就是高频的数据，很少会有两样资产在比较长的一段时间内始终存在lead-lag。所以时间窗口太长往往也不会得到很有用的pair，甚至会错过很多短期lead-lag很明显的pair。但当然，时间太短像一天两天也肯定不行，就像我现在做的一天的情况，数据量太少，本身除去midquote不变化的tick time就已经比较少了（比如海通的3s快照在只考虑tick time的情况下只剩下了250条数据），数据少了本身correlation的准确性就会影响，因为HY估计值的unbiased还好说，但是consistency就发挥不出来因为数据太少，想想我们之前复现的Brownian motion的时候那个数据点的数量都是上万级别的，当然计算时间也特别长。这就导致了现在可以看出日内的LLR的情况，往往LLR最高和最低的都是些极其不对称的比如什么开盘涨停跌停一类的，而榜首出现完全不是证券类的股票也非常常见，比如出现什么医药类或者电力类的股票。因为一天内的数据太少，价格曲线本身就变化多端，这也很正常。而这是无法通过增加grid的细粒度缓解的，因为grid的细粒度仍然只是在尽可能的返回日内的一个lead-lag的不对称性的平均情况。你一天内价格序列就是很不对称那你grid取的再细也没有意义，不能指望说把grid取细从而实现让LLR更接近一，不可能的。目前的数据由于是3s级别的，（不一样，看到很多不同的tick的快照细度，有的是3s的有的5s还有按分钟的），所以影响就是lag取0.1和0.2一定是一样的，因为不会产生更多错位相交的interval，然而他们和0的lag都不一样，**因为领先0.1s或者落后0.1s显然会产生新的错位相交的interval**（比如两个快照数据同频的情况下）。因此我使用的grid一开始是0.5s为单位的，后来就逐渐的拉宽拉长，最大到300s，细粒度已经相当足够了，相当于每个LLR我都要计算71个HY估计值，lead的35个，lag的35个，还有0的一个。

所以，如果这个LLR想用到实盘，理想的情况是算力足够支持，可以实现外层是比如20天左右，然后内层四千乘四千，然后grid根据具体的数据频次而定，如果是更高频的比如一秒钟两个数据（在0.5和1的位置），那grid颗粒度要更细致一点以确保反应情况，比如0.1和0.2是一样的，但是0和0.1和0.5和0.6都会是不一样的，而如果一秒两个数据而且并不是在0.5和1的时间点，而是一个随机的时点，那这个时候0，0.1，0.2，0.3，0.4，0.5……这些的结果都会不一样，这个时候就需要用到文章里面的那个grid去做了，文章里面的数据应该也是这样的。那么这个时候考虑并行的问题，不出意外还是得针对股票pair并行因为数量太大，而另外两个循环就尽量去做成向量化，包括HY可能的进一步的优化，做成向量话，包括grid的for循环使用map的迭代器去替代，包括天数也大概率可以这么做，或者一种想法是先把20天数据拼起来然后一块算，等等。但是目前的问题还是HY计算太慢，哪怕是使用tick time已经大大减少了数据量，基本上96个核并行的话海通筛后一天剩下两百多行，也需要30min，而中信如果是24核并行要3个小时。不敢想象怎么可能去四千刷四千，而且还是二十天，因为我现在已经在用dask/parallel在并行，grid的计算也是通过map的迭代器形式去算的，就算再加速HY函数向量化感觉也不现实，毕竟计算grid要几十个HY的事实摆在那里……

**目前的问题**
1.	A股的数据用什么时间段的？这个数据是怎么生成的？为什么下午三点以后还有，以及14：57的数据是什么情况，还有早上开盘前的数据是什么情况？
2.	Grid怎么取？如果是日内的，或者不同时间段的grid，应该取不一样的面板。要不要带端点？？不同时间段的grid频率应该不一样，比如半小时单位的和一天的，显然一天你的应该更多的grid因为不稳定。同时grid还要根据数据的频率来定。比如你3s的快照那就是grid取0和0.1会不一样，因为数据会错开，相同时间段的数据会错开，但是0.1和0.2和0.5是一样的。所以我暂时把grid按0.5来分组取了。
3.	涨跌停怎么处理？为什么开盘涨停的情况下这个midquote还能发生变化（如果不发生变化那么tick time的时间过少我会把它剔除掉），以及还有没有其他什么需要考虑的特殊情况，导致数据不正常需要提前筛选掉的pair？这么取数据的时间段是否合理？
4.	关于HY相关系数的计算：如果参与就算correlation的数据条目太少是应该剔除的，那少于多少剔除？（因为从HY的定义，他是unbiased和consistent，但是需要相邻的时间interval极其的逼近，意思就是说不仅需要我们的数据本身就极其高频以至于可以取模拟连续的情况，更要求在midquote变化筛选的tick time同样要满足数据比较高频以至于可以模拟连续的情况！而这个本身应该很困难，因为就算是极其高频的行情，如果不是流动性特别强的股票，你快照再频繁也没有用，因为相邻之间不会出现midquote变化，一样tick time会很稀疏。
5.	关于数据的问题，为什么000857在6月1号的成交量是0？哦因为是指数，指数不能直接买。那问题就是这个获取的stock_list里面，第一，为什么会有重复代码？第二，究竟怎么从里面取全A股的数据？删除掉ETF和指数的数据。我目前就是只筛选的股票代码开头为00、30、60、68、43、83、87的股票，包括什么沪深主板中小板新三板啥的，还有北交所的，但目前看来里面有指数，还有涨跌停，这些都需要处理！（比如去除指数和ETF，很可能数据库里面的指数和对应ETF的编号是一样的，那难道是去除所有duplicates就行了？）一样的可能是一只股票和一个指数，比如000933就是神火股份和中证医药，一个是股票一个是指数……所以可能我直接用server获取得到的就是指数……
6.	TMD相关性高的都是指数，跟指数lead-lag我也不能交易指数啊？？
7.	**选取tick time计算LLR会不会影响我们的投资策略**？因为文中这么做是因为他下面构建策略的时候是预测下一个tick time从而进行操作的。不过我想似乎也应该这么做，他下面给的结论是单纯的市价单没办法cover掉价差，可能的建议是去提高阈值，就是quote变化好几次才取一次，这样降低交易频率，但是accuracy也会下降，吃的价差也会少一些。那既然如此，我们是否应该用tick time？或者再采样松一点，比如隔三个tick再取time？不知道是否应该这么做，因为不清楚我们要构建的交易策略是怎么样的。当然每隔几个tick取是好的，因为计算量会成倍的减少！尤其是要扫过四千多只股票，那计算量会少很多。而且无论数据是多高频的，我都只取相邻midquote有变化的时点，那这计算量其实不会随数据变得高频而升高很多。但是问题就是，如果用tick time计算，比如LLR比较显著，然后我们使用了max lead的corr和对应发生的位置，虽然lead的是绝对的时间，但这个时间能用来实际的时间吗？好像也不影响，反正预测的是midquote发生变化的时候。
8.	**还有就是能否考虑intraday profile进行交易**？之前说比如我用过去20天的数据去算LLR，然后取比如平均最高的或者始终统计显著大于1的对子交易，但是我想A股有这个概念的东西，那lead lag的intraday profile应该会很明显，而且lead lag的关系在日内也很有可能发生变化，比如说一个股票在早盘开盘的时候引领了另一只，但是在下午的时候另一只反过来引领，这不是都有可能吗？所以是否可以考虑比如我用过去20天的9:30-10:00的情况去决定今天9:30-10:00交易的pair，然后每过半小时换一批？感觉这样会更科学，因为像海通的报告里面高频因子的有效性都是按什么全天，早收盘前，中午开盘后，盘中啥的分开测试，发现比如开盘后平均委买变化率选股是有效的等等。感觉是应该这么做的，这样的话数据可能会很少，要注意筛选，如果数据太少计算的correlation是不准确的，要筛掉，同时grid也要调整。但是这样也需要注意，就是数据量的问题，correlation这种东西肯定是数据量大了才会准确，然而这个intraday profile取半小时还行，但要是取比如5min，那数据量太少，尤其是使用tick time的话，就算我想研究开盘前也就是9:30前的一个lead lag情况，但是数据量太少了，虽然有半个小时，但是实际发现mid quote的变化的时间点就几个数据点，根本不足以计算correlation，更何况我们还要计算不同的lag grid下的correlation，那就更不准了。总体感觉策略优化到后期这个intraday还是很重要的，文献里面法国市场本就有intraday profile，而且他们还是中午没有收盘。A股市场中午还要收盘，Intraday profile应该会更明显。
9.	怎么选定pair的问题：再看看论文。除了一方面LLR非常大或者非常小的对子里面一大批都是指数以外，另一批经常会出现毫不相关的股票，比如我跑出来的是证券和医药。这也可以理解，这种leadlag在短期内可能显著，但是如果时间拉长就没有了，可能就只是那一天的统计上的巧合罢了。所以单单一天的LLR也许不可信。而如果比如我们计算了多天的LLR，那么反应的就是一个平均的情况，平均下来仍然有leadlag，那拿来预测是比较科学的情况，不会出现因为是短期的leadlag所以昨天有leadlag但今天没有的情况。当然，这只是其中一个问题。更重要的问题是，LLR就算取了平均，也只能反应一个平均的情况，那就是X leads Y比Y leads X更加显著。然而但依据LLR选pair显然没有意义，好比你一个涨停的股票和一个波动贼大的股票，那LLR是可能很不对称，但是他们correlation很低啊，那也同样不具备任何价值，意思就好比你LLR的分子分母都是极其小的数字，除法除出来一个比较decent的LLR，你觉得有意义吗？能拿来做prediction吗？没有意义啊！所以一定是需要看max lead/lag correlation以及对应发生的时间点的，这个东西，也是侯总用regression做不出来的东西，就是这个最佳的lead lag time。所以肯定是得结合看，**LLR只是一个相对强弱指标，不是绝对的，而corr才是绝对的指标。论文里说：Good candidates are close nodes (high correlation) with red links (high LLR). 也就是lead lag network里面越近的点代表correlation越高，correlation应该指的是本身不带leadlag的correlation，因为epps effect说的就是宏观走势一致的在微观不一致，就好比配对交易，我们做leadlag策略交易的对象一定是价格走势高度一致的但是高频的短期内并不一致的对象（我们期待在短时间内当时间从高频开始降低频率，序列继续维持一致）。当然感觉max_lead_corr或者max_lag_corr也应该考虑在内，而且这两个值发生的时点对交易策略的设计应该是有用的**。具体怎么用还没有想好。
10.	所以对应的问题是Leadlag network怎么画？最小生成树？以及需要同时考虑两个指标，correlation和LLR，甚至是同时考虑多个指标比如max_lead_corr和max_lag_corr要怎么抉择？ 
11.	还有就是程序的问题，就是除了可能的LLR的优化，还有就是我们并行需要输出什么结果，以及怎么样搞会更快一点，因为我怀疑存储到字典会减速，字段太冗长复杂了。
12.	最终问题，怎么从pair到确定对应周期内的交易策略？


现在的问题出在哪？
1.	涨跌停需要额外处理。在我用单日的计算测试的过程中，发现尽管直接剔除涨跌停不会影响HY corr的分子，但是分母仍然是照常相加的！！所以这块应该需要处理，应该是要把对应时间段的数据剔除掉。
2.	跨天究竟应该怎么算？我现在还是感觉，首先如果能够确保单天这个计算的相关性和leadlag是正确的话，那么我应该去不同天按照论文去每天计算然后取平均。如果把相邻两天的数据拼到一起，那么也理应把时间点拼起来，因为这个correlation的unbiasedness和consistency的保证要求就是largest of the mesh grid尽可能的相近，然而隔了一晚上，也就是前一天和后一天的数据没有重合，分子不会计算。但是同样的道理，分母仍然会全部考虑计算，这样就会有问题。所以其实应该是拼起来的，但是第一个是具体怎么拼？是9:30拼到14:57？侯总又说因为晚上会有新的信息，所以两天不应该算到一起？那按照这么说，就是只能每天算然后取平均。当然这一切都需要先保证我写的这个计算单独一天的HY corr是没问题的，这个还要多画图多看行情去验证一下。
3.	究竟是否应该取tick time？取tick time是否会导致画出来的价格走势图和实际的价格走势图是不一致的？也就是算出来是有leadlag但实际上从通达信看出来是没有leadlag的？
4.	为什么时间从两个月减到了一个月，中银证券的效果差别这么大？？这个HY correlation到底能不能用？
5.	计算HY关联的时候的prev_time的问题，处理了涨跌停之后，这个prev_time究竟应该在外面提前算好，然后一起平移，还是仍然可以在函数里面去平移？

**一些反思**
第一个应该是检验单天的HY的计算到底对不对。如果正确并且能用，我才能继续，把涨跌停想办法处理了，然后是写一个每天单独计算的程序去验证一下。

今天测试发现这个东西也并不是完全没用的。目前暂时把涨跌停的改了之后，同时尝试了使用tick time和不使用tick time的效果，目前的发现如下：
1.	上面的那些问题，有些是重要的，有些是不重要的。HY的功能强大之处在于他可以解决流动性不同的问题。试想，一个交易特别频繁，一个不频繁，如果你用传统的插值法，那必然结果是相当不准确的，或者是previous tick，因为你都是需插值，然后大量的都是重复的数值，导致计算指标的时候其实是信息重复的，也就必然会出现更加liquid的那个去lead不那么liquid的那个asset。而HY	解决了这个问题，因为HY的计算方法是算区间覆盖，你无论二者流动性相差多大，不频繁交易的那个asset，他相邻区间的跨度会比较大，这样大区间会和频繁交易的asset里面很多的小区间产生重合，因此不会产生信息流失！所以tick time的数据量少也许不是一个特别严重的问题。但是，涨跌停一定是一个很大的问题。原因就在于，涨跌停的时候，如果我们使用的tick time，那么就会排除掉发生涨跌停的资产的后续数据，然后刚刚发生涨跌停和刚刚解除涨跌停的时候的midquote很大，我们也需要筛除。这样导致的后果就是，我们计算HY相关性使用的区间，对于这个asset X，只会有涨停前的区间，和涨停解除后的区间，而不包含涨跌停和解除涨跌停的对应的端点。这就导致不会有一个区间，跨越X的涨停前和涨停后去和Y去计算交集。然而分母仍然会把Y在这段涨跌停时间内的midquote计算进去！！所以必须要把对应区间的Y的midquote给去掉。不然就会导致分母在增加但是分子并没有参与计算的情况。
2.	那隔夜是否影响呢？我感觉会影响！根据上面的推理，只需要保证不会出现分子分母有一个在一直增加但是另一个没有的情况就行。而隔夜的情况，两个资产仍然保持一起交易，分母也不会出现涨跌停那种凭空增多的情况。但是！会出现第二天早上与头一天最后一笔的一个大跨度的区间，这个区间有影响吗？也许有，也许没，涨跌停由于我对于midquote的大小范围的限制导致不会出现大跨度的区间，但是隔夜的情况一定有，这个隔夜的大区间会和前一天另一样资产的密集数据产生交集，因此这么看是不合理的，因为夜晚是有额外的信息，我们应该是不能让前一天的数据和第二天的产生交集的。但当然这产生交集的毕竟是小部分数据，毕竟收盘后就没有数据也不会产生交集了，感觉问题不是很大，需要处理了可以单独处理一下。这么看最合理的办法还是单独每天算，然后取平均。

**现在需要做的事情**
自己写的版本，把prev_time的位置搞清楚，把隔夜和涨跌停的问题彻底解决了。然后开始部署基于那个内置函数的并行版本的代码，并行的逻辑肯定都是要全部重写的，因为这个时候并行的对象就不是股票对了，当然期待做四千扫四千的时候可以用并行。一个扫四千就直接是矩阵拼接就ok，中途可以自己想想可以怎么加速，numpy，numba永远的神。还要搞清楚关键问题，就是为什么涉及到隔夜计算内置函数和自己写的版本结果不一样。这个必须解决清楚，然后再去说是否应该单独按天算，还是单独上下午算取平均，还是可以合并算，当运行速度大大加快的时候，这些东西都可以开始研究了，包括intraday profile，比如我们只关注早盘的leadlag现象，只用早盘的半个小时等等。这就可以研究了，当然还是得先搞明白代码究竟哪里会不会有问题才行。
改用对数价格重新跑。

其实新版的设计思路已经有了，除了可以对四千×四千的做并行，HY的第一个参数传入的是一个series的list，那这就很方便，简单的map得到的结果就是list，或者直接dask并行或者parallel并行得到的结果本来就也是一个list！那就四千乘四千的时候，对四千个做并行，然后对每一个，处理另外的四千个的数据的时候也可以并行。具体来说，我还是打算把需要的所有股票的数据给并行处理好，然后每次对一个，取另外四千个的数据，另外四千个就并行把server time给平移了（甚至这都不需要并行），然后就一个list传入HY的函数就可以了，感觉应该也不需要，因为就一步操作……

现在问题是似乎需要重写两个版本，一个是天求平均的，一个是一起算的，我TM……这个HY应该需要天求平均！这样可以对日期并行？

**自己更改过的模块记得调出来改到我自己的电脑上……包括binance等源代码，还有transport.py里面的参数，还有HFHD里面的HF全部！！到时候记得发回来……**

还有一个目前非常严重的问题，除了那个HY接口处理不了剔除涨跌停和隔夜情况的大区间交集的问题意外，更严重的是我发现，是不是应该每次计算lag的时候都应该重新剔除一次涨跌停啊？？？我傻了，我现在是对原始数据剔除涨跌停对应的数据之后才去计算lag的，但是这样科学吗？以及我写好了函数但是因为涨跌停的存在不能加速，按理来说大批量加速计算可以实现，可以很快，一只扫三百只可以两分钟就完成，前提是数据提前处理好，现在问题就是每个pair都要去对涨跌停处理，主要问题就在涨跌停，导致我不能把几百个asset放到一个list里面直接计算HY关联。烦死了，这个涨跌停到底怎么弄，我干脆剔除这些天数算了操。

**汇报**
1. 最新的结果。Tick time和非tick time的区别（看visualization），然后每日取均值和跨天算的区别。
2. 涨跌停的问题，我目前对涨跌停怎么处理的，以及对隔夜的怎么处理的，然后究竟应该先剔除再lag还是先lag后剔除涨跌停？
3. 并行程序如何设计的问题。如果要考虑涨跌停就寄了。包括最新的接口的问题。

**数据储存优化**
Redis数据库安装，尝试减少读取数据的时间。Linux的sudo apt update无法使用，无法安装ubuntu的apt命令？是权限的问题吗。目前redis还没有跑通。


#### 2.5 2022.06.27进度总结
目前因为侯总没来公司，整整两周没有汇报，进度停滞，不知道接下来干什么。目前阶段的HY的检验部分基本已经做好了，已经可以看接下来怎么设计策略了，我自己也在试着写一些模型预测，包括试着用babuqtuant的套利程序去试一下能不能搞，但是现在想来估计还是得写预测的程序，因为leadlag产生的套利机会，如果是按照huth的文章的交易策略，那就还是在做单边交易。这样的话leader的收益率序列/价差序列就完全是一个lagger的一个因子，与lagger的历史盘口因子一起参与预测就好了。所以如果侯总再不来，我就只能要么试着把babyquant的框架和之前写的那个预测的HFT project框架结合一下写一个机器学习的预测框架，要么就试着写一个实盘交易下使用HY的框架，就按苑总说的那样累加计算看效果。要么就只能继续多看几篇套利的paper，然后试着做一下了，关于HY的还有一些paper，里面使用的策略可以读读看能不能用，另外就是其他的套利的paper了，读一读正好也算准备下面试。实在不行就只能去找吴哥去要点任务做了。

周二要是侯总再不来就先看paper吧，明天把HY的一篇文章，还有其他几篇经典的套利文章都看了，然后在这里做一下笔记，同时看有没有可以复现代码的部分。

**大概总结下现在HY这部分的问题和成果**

**成果**
- 现在最大的成果就是HY的计算非常优化，在工具包中写入了自己的函数，现在可以计算多个序列之间两两的HY，同时可以计算一个asset扫其他一堆asset的HY，这样方便计算lagged的HY关系。并且整体代码用向量化的速度非常的快，可以输出平均计算的结果，也可以输出对应的标准差。也就是说如果认为每天算取平均可以接受，同时涨跌停就去除，那么这样计算就是很快，甚至在下一步的并行算法进一步优化，或者想办法可以用gpu训练的时候，甚至可以实现四千扫四千。目前尝试的是单纯增加数据的时间长度并不会太加长运行的时间，也就是取很多个交易日取平均，甚至说取半年，都不会让速度变慢太多，主要还是参与计算的资产的数目以及并行的方法。
- 在取平均的时间段足够长，比如一个多月，并且使用的不是tick time的情况下，可以实现比较精准的衡量，基本上榜单前几名都是同板块的（证券是这样，但好像换了银行什么的结果不太是银行？？）总而言之，无论怎么说，肯定是可以代替一部分的之前leadlag的计算，就是单纯使用线性回归和lasso回归做的部分的效果。虽然我也不知道HY这个东西的效果有没有回归做的好，毕竟回归的逻辑又简单又实用，比如就单纯拿某种资产的future return对其他资产的历史return回归，可以分别回归看系数显著性，或者直接跑一个多元lasso，然后看剩余的系数如何，未来一段时间就根据这个系数去交易。我猜大概就是用target的过去一段时间的future return与全市场的return跑lasso，lasso完之后留下显著的asset，一般就是同一个板块的，然后target asset再对这些用lasso选出来的asset分别做一个线性回归，然后用回归的系数做预测，比如用选出来的这些asset的return分别做一个target asset的future return的一个预测，然后对这个预测取平均值作为估计决定是否买入卖出等等。这样感觉是比较科学，但是这种回归一定是要插值的，甚至要用一些EWMA去平滑，**然后跑线性回归，因为线性回归的系数本质就是一个插值后同步的一个correlation**。
- 现在就是说一个本质，用HY最差的情况可以替代掉一部分之前用回归跑的结果，其实HY关系可以说本质上就是替代了一元线性回归的那个coefficient，因为一元线性回归的coefficient就是相关系数，用这个相关系数做leadlag的衡量或者做预测，那就可以把对应的这部分换成HY correlation（当然多元lasso这种筛选的功能就不能只用HY实现了，因为HY只能两两的算，而且是否显著的判断也需要另外独自添加判断条件）。总而言之，就是HY的作用和这个线性回归的作用很相似，lasso回归也可以通过HY和LLR的一些东西替代，但当然1.效果是否会比回归好，以及具体怎么做这个筛选，那完全是新的调参的空间，现在就是需要侯总的指导，这个东西我自己不知道怎么做；2.怎么拿来预测，选定了leader的asset以及对应的权重，那可以考虑用leader的return做预测，用correlation去预测，但是这个预测的方式也是不一样的。比如线性回归你就是用那个系数去预测，要么一个leader一个一元回归然后取平均，要么一块一个多元回归做prediction；但是HY预测就可以使用不同lag time的系数做预测（或者只用significant lag），**这就相当于每一个leader，都在跑一个多元回归的prediction**。最后可以取平均或者怎么用的。
- 再说一点关于LLR目前的理解。LLR本身就是一个衡量bi-direction的leadlag的一个相对强弱指标。他本身的作用并没那么明显，因为我们的目标如果说是去交易一个target asset Y，那么我们就是要找现在在lead Y的这些资产，那可能我们可以只挑选那些lead Y比Y lead他更显著的资产做预测，也可以把所有对Y存在lead的（也就是不管LLR多大）的都用来参与预测。而如果我们考虑的是交易一个对子，比如X lead Y更显著的时候交易Y，而反过来Y lead X的时候交易X这种策略，那LLR是有用的，不过感觉这种应用场景很少。

**总结一下就是HY相比回归的缺点和优点**
- **回归的优点**：回归速度快，原理简单而清晰明确，lasso用来选有用的leader，回归系数用来做prediction。实现起来很快，适合实盘；**回归的缺点**：中间需要插值，并且一元回归的本质就是一个synchronous的linear correlation，缺点类似previous tick，甚至不如previous tick（甚至可以考虑用previous tick替代一元回归的系数），就是算出来的是biased，Inconsistent，并且会收到micro noise的影响。最严重的缺点还是会出现总是比较liquid的那样资产在始终lead不是那个liquid的资产，因为不是那么liquid的资产在计算的时候会不停的插值interpolation，然后回归就会导致这个情况。就算使用EWMA做平滑仍然不会有很好的缓解。当然，如果是A股，并且两个交易频率相近，而且用original time，都是快照数据比如3s的话，那数据的采样频次差不多，影响就不会那么大，毕竟我们tick数据都是快照的，频次接近。但如果使用的是tick time，那流动性的问题就会马上暴露出来。
- **HY的缺点**：计算慢，不知道怎么用（包括不同time lag的系数怎么用来预测收益，取多长的时间计算，多久rolling一次，多久更迭一次，LLR怎么用，怎么筛选leader等等都是需要确定的问题），甚至可以考虑一部分用HY，剩下一部分还用回归，比如使用lasso做筛选，然后用HY的系数去做预测这样因为HY的筛选机制不明确等等，手头有不同lag的HY系数，怎么用，怎么算，怎么做预测，怎么做策略，都不知道。**HY的优点**：最大优点就是数据足够高频下反应的是真实的时序相关性，同时自动处理异步性的问题，以及可以提供很多的信息，就是不同time lag下的一个系数，这些系数往往都可以提供一些信息，**能具体告知lead几秒或者lag几秒肯定是很有用的信息**。只是怎么用，现在不知道罢了。还有一个很显著的优点，就是现在计算HY的时候用的是内置的函数，里面是有一个阈值参数的，这个参数很有用，甚至说对流动性不同的asset会专门有推荐的值，比如0.4-0.6一类的，这个参数显然也可以调整和优化，如果需要用来预测的话这个参数是肯定有用的。



**问题**
- 部署的并行程序还需要优化，比如试过是不太能一个parallel里面再套一个parallel的。这样反而核数都没用起来。在真正需要几千扫几千的时候需要根据实际情况再写并行程序。包括计算股票数据的时候，可能还需要根据date和lag的相对的一个数目大小决定对哪个并行，然后只能对另一个用迭代器，**至少目前没有想到一个更好的办法，能让他们全部并行起来的办法**。而且之前跑股票数据的时候对日期并行好像一直跑不了。
- 涨跌停和隔夜的问题。涨跌停：是否可以直接剔除，不处理；如果处理的话，需要每一个lag都处理，也就是之前的代码还是有问题的，不能先处理好然后分别计算lag，这样大的时间跨度仍然是会cover的，要么就选择直接不处理，把对应的天数扔掉，要么就选择忽略这个大的区间跨度。隔夜：我自己写的代码是可以处理隔夜的问题的，就算计入了lag应该也是不会产生大的区间的，但是如果要使用built-in的HY方法那就不行，那种方法只能算连续时间的日内的，因为需要预先把时间输入进去，然后他去做匹配，在内部去做shift。总而言之从各种时间精力的角度考虑不如就直接计算日内取平均。
- 日内算取平均，整体算，以及上下午分开算的结果还是很不一样的，不过时间足够长的话结果是相似的。但是使用tick time和original time的结果是完全不一样的，甚至原先lead的在tick time下直接就是lag的榜首，这个还是很不一样的，所以需要看到底要用哪个，哪个才能拿来用来做套利。目前看的文章里面都是在用tick time，以及对应的这个correlation做预测，但tick time跑出来的结果就是证券板块的前几名就不太会是证券，当然时间更长的没试过，一两个月的时间前几名不是证券就已经比较奇怪了。

#### 2.6 一些完善性的研究
**全A股聚类研究**
仿照sklearn的那个similarity measure的tut，使用各种聚类方法，包括熟悉一下类似多元高斯分布correlation matrix估计的图lasso方法，练习一下机器学习。

**Granger因果检验**
statsmodels实现。仿照HF包里面获取上三角matrix的index的方法去计算并获取p检验值，statsmodels的检验是需要pairwise进行，然后检验的是dataframe的第二列对第一列产生granger因果的显著性，所以implement的时候需要pairwise并行并且注意列排列的顺序。

**总结下similarity measure与granger test的必要性**
本打算花更多的时间尝试对数据填充匹配对齐，然后尝试严谨的cluster，做similarity衡量，然后做granger检验，然后做一做lasso回归的预测。现在想不如更多的时间花来做模型来赶紧学点深度学习为下一段实习蓄力。也算是把策略回测前的工作做的尽可能的完善了。总结一下:

在我现在看来cluster和一些常见的similarity measure仅仅是初步的一些分析,如果数据的时间线长一些或者使用的日线数据,那么基本上cluster是能反应很多有意思的信息.对于correlation matrix的衡量现在有太多的手段,比如线性系数/HY系数矩阵/对高斯序列用的图lasso等拟合技术/dynamic time warping等各种方法.正好有affinity propagation这种方法可以测试这些correlation matrix的准确性.当然,这么多的方法衡量两个时序的相关性,很有用,能给我们关于原始时间序列的基本认知,但是都仅限于原序列.我们的最终目的是预测,这些工具只能近似告诉我们原始序列或者说偏宏观上的相关性.所以HY相关系数或者lasso regression的作用仍然是无法替代的.当然在用这两种工具做leadlag的预测的时候,提前使用granger因果检验是必要的,granger也是基于原始序列,并且不能给我们有关预测的帮助,但是granger能告诉我们究竟能不能用X来预测Y.如果可以,如果能,那我们再去用lasso回归,再去用HY相关系数做预测.毕竟本身HY系数大小和LLR这种东西不能告诉我们序列X对Y的预测效果究竟是怎样,因为缺乏一个严谨的statistical test.而这就是granger test的意义所在.


#### 2.7 基于Leadlag的统计套利策略本质

- Leadlag策略的本质是统计套利。统计套利说的是我们利用现价和fair price的差值进行套利，统计是用来计算fair price的统计模型。而leadlag策略本质就是利用股票对的信息差距，我们用快的那个去预测慢的那个资产在此时此刻的fair price，然后利用现价和fair price地方差距进行套利，我们用的预测lagger的fair price的模型就是使用leader的一个线性回归模型（也可以是其他模型），然后等到ticker时间到，lagger的价格回复到我们的fair price，我们就可以获利，所以leadlag策略本质就是统计套利。

- **Leadlag是最能解释为什么统计套利的本质是做趋势。我们始终在用leader去预测lagger，交易也只是交易的lagger，决定了pair之后的任务就是对lagger做单边的趋势，就是在做日内的CTA，就是在做趋势预测，做择时。**

- <b><font color=blue>Leadlag策略在赚什么钱？在套什么利？<font color=red>做leadlag策略交易的对象一定是价格走势在低频上高度一致的但是高频上的短期内并不一致的对象</font>，因为根据epps effect，高度相关的两样资产在微观上的相关性会降低。数据切片频率越高，相关系数越低。因此我们交易这种价格走势上宏观一致的但短期不一致的，那么在高频时点二者对信息的反应并不能达到同步，相关性会比低频数据算来的要低，此时就会产生lead lag，然后在我们预测做出反应的时候，随着时间拉长，两者的价格走势趋同，序列继续维持一致，回到fair price，我们就可以套利，这就是leadlag在赚的钱。</font></b>


#### 2.8 如何构建预测策略

**文章的套利策略以及可能的优化**
- 具体实现就是先滚动的回测，滚动的选lead-lag pair，然后去交易，pair的选取/调仓好比选股策略，比如每周一调，不过一般应该还是每日平仓调仓，每日更新pair，甚至可以像前面说的更加的高频，就是每日考虑intraday的LLR和correlation的profile，比如根据不同时段早收盘前，中午开盘后，盘中分别调仓使用不同的pair，那么相应的回测选取pair也应该是前若干天滚动的对应时段的数据计算的correlation。

- 具体怎么选pair，是一个大难题。知道要考虑LLR，但更要考虑本身的correlation要高，同时可能也需要考虑最大的lead correlation和最大的lag correlation，具体怎么选取，文献里只给出了network的用法以及一个很模糊的说辞，只是说都要考虑，具体怎么用还得自己开发研究。

- 选好pair之后在对应的交易期间内（比如一天内、不同时段内），要做的就是趋势策略的搭建。最简单的根据文章中的，使用leader的return跑一个线性模型去预测下一个tick time的lagger的return的涨跌，做一个二分类，回归预测的return大于零就涨，小于零就跌。然后每次预测结果出来都去交易（因为做的不是回归不知道具体涨多少跌多少），按市价单去开多单空单（即在ask1去买在bid1去卖）。当然这里能优化的太多了，一个是预测的因子太少，预测的模型太简单，而且开仓平仓的时点也很粗糙，凡是预测完就开仓，凡是到预测的下一个tick time就平仓。

- 如同文章所说，我们计算correlation和预测未必使用tick time，可以是多个tick time。预测涨跌或者预测具体的变化都可以，预测涨跌就是文章里面那个二分类问题，只关心符号正负（文章面对分类问题仍然做的是回归，因为只要回归值大于0就是涨小于0就是跌）；预测具体midquote的变化就是回归问题，而我们可以设置比如预测midquote change大于一定的阈值我们再开多和开空，否则不动。**根据预测midquote变化超过多少阈值再开多开空也就是调整开仓的时点，然后根据预测的是多少个tick time后的收益率去决定平仓的时间点**。总之就是开仓的时点、平仓的时点、HY correlation计算的时点都可以考虑加上阈值的版本。
  
- 止盈止损，这里就不说了。
- HY系数本身带有一个threshold参数，这个参数显然可以用上。
- 如果只使用leader的收益率序列做线性的预测，文章里是用所有显著的time lag，然后对不同的time lag的leader的return的跑一个线性预测。那么预测使用的系数是什么？文章中应该是过去20天的均值，我个人认为要么就用前几天的，要么就滚动的算，像那个实盘的框架一样直接滚动的计算这个系数，这显然也是一种优化，而且贼方便的是不用rolling，直接做一个动态增加的框架然后实时的更新系数即可，**相当于整个HY不同lag的系数和LLR都在一个动态更新的过程**。然后如果用的是tick time，就每次预测到下一个tick的收益率后，比如如果收益率绝对值超过了手续费，那就开多或者开空，然后在预测的下一个tick的时点平仓。如果使用的是original的快照时间，那么可以考虑每个快照都预测下一个快照，然后超过阈值开仓，然后到下一个快照就平仓。当然这样频率太高，也可以考虑像原先paper说的一样对原始数据点做一个采样（这个采样也是babyquant经常做的事情），然后预测下一个采样点的收益率，这样的开仓平仓的时间间隔就稍微长一些，不至于交易太过频繁，或者就像babyquant一样设置什么开仓平仓阈值，阈值高一点自然交易的次数就少一点，**比如我们认为预测的收益率超过两倍的spread才开仓等等**。**再详细一点，假设X是leader，Y是lagger，那么是每次Y到了一个行情的时候我们才去立刻做prediction还是根据X的节奏每次X来一个行情的时候做prediction？按理这个过程没用到lagger的任何信息，所以理论上是有了X的行情就可以立刻更新对Y的预测，所以理论是X产生了行情就预测，然后预测有信号了就下单？是这样吗？**

**第二种套利策略**
- 在另一篇HY的衍生paper中给了另一种的交易策略。把每个连续的X行情和Y行情放到一个cluster里面，X和Y就是midquote价格，可以用对数价格但文章就是价格，那么每个价格时间点都可以计算对应时点相对于上一个时点的差值，就是价差，**当一个cluster内存在一个收益率或者对数收益率的绝对值大于一定阈值的时候，产生交易信号（1代表开多，-1代表开空，没超过阈值就算0就是不开仓），而产生交易信号后立刻开Y的仓位（这样的话和上一种方法都是在X出现行情的时候判断是否有信号，有信号就立刻开仓），然后一直持有到一整个Y的cluster，直到X下次行情出现就平仓**。这种方法相比与前一种，操作性更强，逻辑比较简单，实现起来应该也更简单，PNL很好计算！然而可能也有一些问题，比如根本就没有用到不同time lag的correlation的信息，**而是完全以cluster作为单位，根据leader的涨跌来预测是否产生交易信号，相当于默认X lead Y就是X在领先Y一个cluster这个样子**。**统计accuracy的时候也是判断X的cluster整体的价差是否和下一个紧接的Y的cluster的整体价差同号**。那如果leader的lead corr并不是很显著，这样显然就会出现一些误判的情况。所以本身这也属于一种naive strategy。感觉甚至太过naive，因为完全就相当于选择了pair之后就完全没有用到任何历史的HY corr的信息……这里能调的就只有开仓的对leader设置的阈值K，以及每次开仓的合约数量。
- 对于第一个参数，每次有信号开仓的数量，开仓的数量可以是固定手数，但同样可以根据预测值绝对值的大小，第二种策略就是根据X的cluster内的收益率的绝对值大小来决定开仓的仓位。这也是一个优化空间。第二篇文章提到了一次信号只交易一张合约的话不用考虑market impact，倒也是。如果增加单次交易合约数目，手续费可能会吃得少，因为手续费可能不是按合约张数扣的，而是按order扣的。但是market impact也会变大，所以就是5530学的一个trade-off，不能太大不能太小。
- 对于第二个参数，阈值K，类似这种开仓阈值或平仓阈值，就应该像babyquant一样去做网格优化。这里不涉及平仓阈值，就一个开仓阈值，显然很好做优化，使用不同的(lead, lag)交易对的最佳K不一样，K太小，很吃手续费，K太大，交易太不频繁，降低收益。所以又是一个trade-off。同时还发现了K太大还有另一个风险，每次predict的return或者X的return比较大的时候，对应的reaction time会大幅下降，也就是交易机会更加转瞬即逝。
- **这种交易方法虽然简单愚蠢，但是优点就是计算极快，因为交易信号的产生只需要做一次Logical comparison，运算极快，也正好比较适合超高频交易**，就像第二篇文章里面这种，做的就是跨国际期货交易所套ETF这种需要超高频的数据，超高频的信号的交易。的确如果是这种超高频的数据和信号需求，就算是tick time，我们前面那种计算HY的方法也吃不消，哪怕不是滚动而是从开盘开始累加，也需要计算interval是否overlap，大概是三次的for loop，支持不起毫秒级的计算需求。



**趋势策略**
- 可以使用更多的特征，我们一般不会只使用**leader的midquote change来做预测对吧，说的简单些，leader的midquote change序列，就相当于我们挖掘的针对于lagger交易的一个重要因子**。仅使用leader的序列预测lagger，相当于我们在做一个单因子的回测，做一个单因子的趋势CTA。那我们显然可以把这个单因子和其他的因子合成起来。leader本身就有很多，显然可以都用上。或者比如在不知道这个lead-lag的时候，我们肯定会用这个lagger本身的各种历史价量信息，各种技术指标，还有各种比如订单簿因子成交因子委托因子去做时序预测，那我们就可以把这些因子合成起来，然后给leader的return序列加一个高的权重，做一些什么标准化啊，预处理，包括因子正交，还可以进一步做遗传规划因子挖掘，然后去跑回测。而且我们当然也不会只用leader的return这个单因子，可以用这个字段去生成price序列有关的各种因子，也就是leader return序列的技术指标因子、depth ratio，各种什么偏度因子、峰度因子，包括babyquant使用的各种EWMA，双均线合成因子、波动率因子等等。同样也可以进一步使用统计函数做leader因子挖掘，还可以leader和lagger的数据结合在一起挖掘，思路很多很开阔了。

- 可以使用非线性模型。尽管LLR本身的推导是依赖于线性模型的，但是LLR的作用只是用来选pair，而并不是用来做预测做交易。他只是告诉我们leader的return序列很有用。我们完全可以拿来，结合上面的那些因子，去喂到各种机器学习模型里面。其实到了这一步，基本上就和babyquant的CTA是一致的了，因为babyquant的CTA就是在做单品种的交易与回测，而leadlag本身就是在做单边的趋势策略啊！那就是完全一样，可以按照babyquant的方式去生成上面说的那些因子，然后还可以做因子合成，就是方向因子乘上趋势因子，然后喂到各种机器学习模型，线性回归、lasso、ridge、elasticnet、adaboost、GBDT、xgboost、lightgbm、RNN、CNN、MLP等等……做出预测然后交易。这本身可以是一个分类问题也可以是一个回归问题，反正预测的是下一个tick time的Midquote的涨跌，二分类问题或者回归问题，那就是机器学习的天堂。


#### 2.9 2022.07.06下一步的规划
目前leadlag进度停滞，需要进一步的指导去开发日内的趋势预测，主要是完全没有这方面的经验，必须有leader指导啊。再加上也同时段在看套利的文章，也在学期货的CTA，现在是7月6号，突然感觉紧迫了起来，实习已经过去快两个月了……如果真的要考虑月底新申请或者8月换实习，我得保证这段实习有充足的经历，学够了东西，做够了充足的project才行啊。现在实打实过去了快两个月了，能拿得出手的只有一个project，而且还没有完成最后的策略设计。现在必须开始新的project了，毕竟上一个project已经做了我力所能及的一切了，需要指导才能往下做。我打算把重心放到做模型上面了。本来我来这家公司，想做深度学习时序预测，本来就是做模型，想着借这个机会把之前学的各类ML和DL方法投入实践，但是没想到给我说没卡。但没卡也不影响我做模型，而且后面找实习一定很看重个人机器学习和深度学习的能力，我想趁这段正好有时间，自己把学习重心放到ML和DL上面，包括各类time series model，LSTM，transformer，CNN，图神经网络GNN等等，因为我大四应该也不会再花时间上FTE4560这种课了，所以接下来的机器学习进阶和深度学习就要全靠我自己自学了，我会在大四下选上强化学习DDA4230，然后final project做一个强化学习的交易系统，这方面的project在github肯定是很多的，花一门课的功夫把强化学习的经典模型都学了，包括机器之心SOTA里面的各类RL模型。总之DL接下来的部分是要我自学了，图神经网络和语言模型这些东西，NLP，估计要等到硕士才能上到课，所以只能自学。那就把重心全部放在ML、DL的学习上吧！希望到大四下结束我能MLDLRL三修，实习一份接一份。
这周前两天借机会把statsmodels和cluster复习了一下，现在ML的部分感觉还算好，打算主攻深度学习。目前有三个想做的事情：
1. 先复习好以前自己写的MLDL模型的slides和code，然后去gitub把pytorch的tutorial刷一遍，然后做好笔记，再去做接下来的部分
2. 自己开发套利策略，就复现文章中的那个多空预测框架，文章有代码，里面也有模型预测的部分，就在这部分开发自己的DL预测
3. 自己开发CTA策略，把期货的CTA用到股市上，按照期货CTA的流程走一遍，接这个机会正好把之前没理解的各种框架问题都理解清楚

以上相当于套利和CTA两个方向开发的大project了，希望我能努力做完。另外更希望的是把以前学过但从没用过的东西全部复习整合起来，包括HFT的那个project，Kaggle的optiver大赛，里面用到的方法架构完全可以在模型这一块应用起来，开发更多的因子和特征。还有以前整理的DL的tut，做过的project，现在都捡起来，看学术论文，然后去github找代码，复现，优化改进。

当然，以上三个部分都是在闲下来的时候去做的，侯总要是在指导那还是按他的要求去做，我还是希望从他身上学到更多东西的，包括套一些策略思路和研究思路等等，毕竟光靠我一个人摸进步太慢，拿不到推荐信，下一段实习也很难找。还是要主动一点，既然回来了就主动去请教。最近变故有点多，侯总生病要治疗一个月，这边吴哥又打算走了，也许这是个机会，因为之前一直说我没有卡来训练，等吴哥走了之后是不是我就可以用卡了？但是我现在实在深度学习的水平太拉跨，就算给我卡也不会做，不如我打算就最近好好准备gre，然后好好把深度学习transformer一类的东西做了，等回来了也许我就有机会做模型了。无论如何，希望度过这个艰难的时候吧。
